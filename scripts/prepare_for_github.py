#!/usr/bin/env python3
"""
GitHubæ¨é€å‡†å¤‡è„šæœ¬
æ¸…ç†é¡¹ç›®ï¼Œåˆ é™¤æµ‹è¯•æ–‡ä»¶å’Œä¸å¿…è¦çš„æ–‡ä»¶ï¼Œåªä¿ç•™å®éªŒç›¸å…³çš„æ ¸å¿ƒæ–‡ä»¶

ä½œè€…: å¼ æ˜• (ZHANG XIN)
é‚®ç®±: zhang.xin@duke.edu
å­¦æ ¡: Duke University
"""

import os
import shutil
from pathlib import Path
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def clean_project(project_root: str):
    """
    æ¸…ç†é¡¹ç›®ï¼Œåˆ é™¤ä¸å¿…è¦çš„æ–‡ä»¶
    
    Args:
        project_root: é¡¹ç›®æ ¹ç›®å½•
    """
    root = Path(project_root)
    
    # è¦åˆ é™¤çš„æ–‡ä»¶å’Œç›®å½•åˆ—è¡¨
    files_to_remove = [
        # æµ‹è¯•æ–‡ä»¶
        "test_dataset_loading.py",
        "experiment_log.txt",
        
        # å¤‡ä»½æ–‡ä»¶
        "src/utils/data_loader.py.backup",
        
        # ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜
        "__pycache__",
        "*.pyc",
        "*.pyo",
        "*.pyd",
        ".pytest_cache",
        ".coverage",
        "htmlcov",
        
        # IDEæ–‡ä»¶
        ".vscode",
        ".idea",
        "*.swp",
        "*.swo",
        "*~",
        
        # ç³»ç»Ÿæ–‡ä»¶
        ".DS_Store",
        "Thumbs.db",
        
        # æ—¥å¿—æ–‡ä»¶
        "*.log",
        "logs/",
        
        # ä¸´æ—¶å®éªŒç»“æœï¼ˆä¿ç•™resultsç›®å½•ç»“æ„ä½†æ¸…ç©ºå†…å®¹ï¼‰
        "experiment_figures/",
    ]
    
    # è¦ä¿ç•™çš„æ•°æ®é›†ç¤ºä¾‹æ–‡ä»¶ï¼ˆå°æ ·æœ¬ç”¨äºæ¼”ç¤ºï¼‰
    keep_sample_data = True
    
    logger.info(f"å¼€å§‹æ¸…ç†é¡¹ç›®: {root}")
    
    removed_count = 0
    
    for pattern in files_to_remove:
        if pattern.endswith('/'):
            # ç›®å½•
            dir_path = root / pattern.rstrip('/')
            if dir_path.exists() and dir_path.is_dir():
                shutil.rmtree(dir_path)
                logger.info(f"åˆ é™¤ç›®å½•: {dir_path}")
                removed_count += 1
        elif '*' in pattern:
            # é€šé…ç¬¦æ¨¡å¼
            for file_path in root.rglob(pattern):
                if file_path.is_file():
                    file_path.unlink()
                    logger.info(f"åˆ é™¤æ–‡ä»¶: {file_path}")
                    removed_count += 1
                elif file_path.is_dir():
                    shutil.rmtree(file_path)
                    logger.info(f"åˆ é™¤ç›®å½•: {file_path}")
                    removed_count += 1
        else:
            # å…·ä½“æ–‡ä»¶
            file_path = root / pattern
            if file_path.exists():
                if file_path.is_file():
                    file_path.unlink()
                    logger.info(f"åˆ é™¤æ–‡ä»¶: {file_path}")
                    removed_count += 1
                elif file_path.is_dir():
                    shutil.rmtree(file_path)
                    logger.info(f"åˆ é™¤ç›®å½•: {file_path}")
                    removed_count += 1
    
    # æ¸…ç†æ•°æ®ç›®å½•ï¼Œä½†ä¿ç•™ç»“æ„å’Œå°æ ·æœ¬æ•°æ®
    data_dir = root / "data"
    if data_dir.exists():
        logger.info("æ¸…ç†æ•°æ®ç›®å½•...")
        
        # ä¿ç•™processedç›®å½•ç»“æ„ä½†æ¸…ç©ºå†…å®¹
        processed_dir = data_dir / "processed"
        if processed_dir.exists():
            for subdir in processed_dir.iterdir():
                if subdir.is_dir():
                    # ä¿ç•™ç›®å½•ç»“æ„ï¼Œä½†æ¸…ç©ºå†…å®¹
                    for item in subdir.iterdir():
                        if item.is_file():
                            item.unlink()
                        elif item.is_dir():
                            shutil.rmtree(item)
                    logger.info(f"æ¸…ç©ºprocessedå­ç›®å½•: {subdir}")
        
        # å¯¹äºrawç›®å½•ï¼Œä¿ç•™å°æ ·æœ¬æ•°æ®ç”¨äºæ¼”ç¤º
        raw_dir = data_dir / "raw"
        if raw_dir.exists() and keep_sample_data:
            logger.info("ä¿ç•™å°æ ·æœ¬æ•°æ®ç”¨äºæ¼”ç¤º...")
            
            # é™åˆ¶æ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬æ•°é‡
            dataset_limits = {
                "coco": {"images": 10, "annotations": True},
                "flickr30k": {"images": 10, "annotations": True},
                "cc3m": {"images": 10, "annotations": True},
                "visual_genome": {"images": 10, "annotations": True}
            }
            
            for dataset_name, limits in dataset_limits.items():
                dataset_dir = raw_dir / dataset_name
                if dataset_dir.exists():
                    # é™åˆ¶å›¾åƒæ•°é‡
                    images_dirs = ["images", "train2017", "val2017", "flickr30k_images", "VG_100K"]
                    for img_dir_name in images_dirs:
                        img_dir = dataset_dir / img_dir_name
                        if img_dir.exists():
                            image_files = list(img_dir.glob("*.jpg")) + list(img_dir.glob("*.png"))
                            if len(image_files) > limits["images"]:
                                # ä¿ç•™å‰Nä¸ªæ–‡ä»¶ï¼Œåˆ é™¤å…¶ä½™çš„
                                for img_file in image_files[limits["images"]:]:
                                    img_file.unlink()
                                logger.info(f"é™åˆ¶{dataset_name}/{img_dir_name}å›¾åƒæ•°é‡ä¸º{limits['images']}")
    
    logger.info(f"æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {removed_count} ä¸ªæ–‡ä»¶/ç›®å½•")

def create_gitignore(project_root: str):
    """
    åˆ›å»ºæˆ–æ›´æ–°.gitignoreæ–‡ä»¶
    
    Args:
        project_root: é¡¹ç›®æ ¹ç›®å½•
    """
    gitignore_content = """
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
PIPFILE.lock

# PyTorch
*.pth
*.pt

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
Pipfile.lock

# PEP 582
__pypackages__/

# Celery
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs
*.log
logs/

# Data (keep structure but ignore large files)
data/raw/*/images/
data/raw/*/train2017/
data/raw/*/val2017/
data/raw/*/test2017/
data/raw/*/flickr30k_images/
data/raw/*/VG_100K/
data/raw/*/VG_100K_2/
data/processed/

# Results
results/
experiment_figures/
*.png
*.jpg
*.jpeg
*.pdf

# Temporary files
temp/
tmp/
*.tmp
*.bak
*.backup

# Model checkpoints
checkpoints/
models/
*.ckpt

# Weights & Biases
wandb/

# MLflow
mlruns/

# Test files
test_*.py
*_test.py
tests/temp/

# Coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/
"""
    
    gitignore_path = Path(project_root) / ".gitignore"
    
    with open(gitignore_path, 'w', encoding='utf-8') as f:
        f.write(gitignore_content.strip())
    
    logger.info(f"å·²åˆ›å»º/æ›´æ–° .gitignore æ–‡ä»¶: {gitignore_path}")

def create_readme_update(project_root: str):
    """
    æ›´æ–°READMEæ–‡ä»¶ï¼Œæ·»åŠ æ•°æ®é›†é…ç½®è¯´æ˜
    
    Args:
        project_root: é¡¹ç›®æ ¹ç›®å½•
    """
    readme_path = Path(project_root) / "README.md"
    
    if readme_path.exists():
        with open(readme_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ·»åŠ æ•°æ®é›†é…ç½®è¯´æ˜
        dataset_section = """

## æ•°æ®é›†é…ç½®

æœ¬é¡¹ç›®æ”¯æŒä»¥ä¸‹æ•°æ®é›†ï¼š

### ä¸»è¦æ•°æ®é›†ï¼ˆå·²éªŒè¯ï¼‰
- **MS COCO**: å›¾åƒ-æ–‡æœ¬æ£€ç´¢åŸºå‡†æ•°æ®é›†
- **Flickr30K**: å¤šæ¨¡æ€æ£€ç´¢æ ‡å‡†æ•°æ®é›†

### æ‰©å±•æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰
- **Conceptual Captions (CC3M)**: å¤§è§„æ¨¡å›¾åƒæè¿°æ•°æ®é›†
- **Visual Genome**: å¯†é›†æ ‡æ³¨çš„è§†è§‰ç†è§£æ•°æ®é›†

### æ•°æ®é›†å‡†å¤‡

1. ä¸‹è½½æ•°æ®é›†åˆ° `data/raw/` ç›®å½•
2. è¿è¡Œæ•°æ®é›†ä¿®å¤è„šæœ¬ï¼ˆå¦‚éœ€è¦ï¼‰ï¼š
   ```bash
   python scripts/fix_datasets.py
   ```
3. éªŒè¯æ•°æ®é›†åŠ è½½ï¼š
   ```bash
   python -c "from src.utils.data_loader import DataLoaderManager; print('æ•°æ®é›†åŠ è½½æ­£å¸¸')"
   ```

### ç›®å½•ç»“æ„
```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ coco/
â”‚   â”‚   â”œâ”€â”€ annotations/
â”‚   â”‚   â”œâ”€â”€ train2017/
â”‚   â”‚   â””â”€â”€ val2017/
â”‚   â”œâ”€â”€ flickr30k/
â”‚   â”‚   â”œâ”€â”€ flickr30k_images/
â”‚   â”‚   â””â”€â”€ results_20130124.token
â”‚   â”œâ”€â”€ cc3m/
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ cc3m_annotations.tsv
â”‚   â””â”€â”€ visual_genome/
â”‚       â”œâ”€â”€ images/
â”‚       â”œâ”€â”€ region_descriptions.json
â”‚       â””â”€â”€ image_data.json
â””â”€â”€ processed/
    â””â”€â”€ (è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†åæ•°æ®)
```
"""
        
        # å¦‚æœREADMEä¸­æ²¡æœ‰æ•°æ®é›†é…ç½®è¯´æ˜ï¼Œåˆ™æ·»åŠ 
        if "## æ•°æ®é›†é…ç½®" not in content:
            content += dataset_section
            
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info("å·²æ›´æ–°READMEæ–‡ä»¶ï¼Œæ·»åŠ æ•°æ®é›†é…ç½®è¯´æ˜")
        else:
            logger.info("READMEæ–‡ä»¶å·²åŒ…å«æ•°æ®é›†é…ç½®è¯´æ˜")

def main():
    """
    ä¸»å‡½æ•°
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent
    
    logger.info(f"å‡†å¤‡é¡¹ç›®æ¨é€åˆ°GitHub: {project_root}")
    
    # æ¸…ç†é¡¹ç›®
    clean_project(str(project_root))
    
    # åˆ›å»º/æ›´æ–°.gitignore
    create_gitignore(str(project_root))
    
    # æ›´æ–°README
    create_readme_update(str(project_root))
    
    logger.info("ğŸ‰ é¡¹ç›®æ¸…ç†å®Œæˆï¼Œå¯ä»¥æ¨é€åˆ°GitHubäº†ï¼")
    
    # æä¾›Gitå‘½ä»¤å»ºè®®
    logger.info("")
    logger.info("å»ºè®®çš„Gitå‘½ä»¤ï¼š")
    logger.info("git add .")
    logger.info("git commit -m 'feat: å®Œæˆå¤šæ¨¡æ€æ£€ç´¢å¯¹æŠ—é˜²å¾¡ç³»ç»Ÿæ ¸å¿ƒå®ç°'")
    logger.info("git push origin main")
    logger.info("")
    logger.info("æ³¨æ„ï¼šè¯·ç¡®ä¿å·²ç»é…ç½®äº†æ­£ç¡®çš„Gitè¿œç¨‹ä»“åº“")

if __name__ == "__main__":
    main()