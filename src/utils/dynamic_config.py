"""åŠ¨æ€é…ç½®ç®¡ç†æ¨¡å—

æ ¹æ®ç¡¬ä»¶æ£€æµ‹ç»“æœè‡ªåŠ¨ç”Ÿæˆå’Œç®¡ç†é…ç½®æ–‡ä»¶ã€‚
"""

import json
import yaml
import logging
import os
from typing import Dict, Any, Optional
from pathlib import Path
from dataclasses import dataclass

from .hardware_detector import HardwareDetector, HardwareConfig
from ..models.sd_model import StableDiffusionConfig
from ..models.multi_gpu_sd_manager import MultiGPUSDConfig
from .multi_gpu_processor import MultiGPUConfig

logger = logging.getLogger(__name__)


@dataclass
class DynamicConfigProfile:
    """åŠ¨æ€é…ç½®æ¡£æ¡ˆ"""
    name: str
    description: str
    hardware_requirements: Dict[str, Any]
    config_template: Dict[str, Any]
    priority: int = 0  # ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜


class DynamicConfigManager:
    """åŠ¨æ€é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = "configs"):
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(parents=True, exist_ok=True)
        self.hardware_detector = HardwareDetector()
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # é¢„å®šä¹‰é…ç½®æ¡£æ¡ˆ
        self.profiles = self._load_predefined_profiles()
    
    def _load_predefined_profiles(self) -> Dict[str, DynamicConfigProfile]:
        """åŠ è½½é¢„å®šä¹‰é…ç½®æ¡£æ¡ˆ"""
        profiles = {}
        
        # é«˜æ€§èƒ½é…ç½® (6+ GPUs, 24GB+ each)
        profiles["high_performance"] = DynamicConfigProfile(
            name="high_performance",
            description="é«˜æ€§èƒ½é…ç½® - é€‚ç”¨äº6+ä¸ª24GB+æ˜¾å­˜çš„GPU",
            hardware_requirements={
                "min_gpu_count": 6,
                "min_gpu_memory_mb": 24000,
                "min_total_memory_mb": 144000
            },
            config_template={
                "stable_diffusion": {
                    "model_name": "runwayml/stable-diffusion-v1-5",
                    "device": "auto",
                    "torch_dtype": "float16",
                    "num_inference_steps": 50,
                    "guidance_scale": 7.5,
                    "height": 512,
                    "width": 512,
                    "num_images_per_prompt": 4,
                    "enable_attention_slicing": True,
                    "enable_xformers": True,
                    "enable_cpu_offload": False,
                    "enable_safety_checker": False,
                    "enable_multi_gpu": True,
                    "models_per_gpu": 2,
                    "max_models_per_gpu": 2
                },
                "multi_gpu": {
                    "batch_size_per_gpu": 8,
                    "max_workers": 12,
                    "memory_fraction": 0.9,
                    "enable_mixed_precision": True,
                    "enable_compile": True,
                    "load_balancing": True
                },
                "detection": {
                    "batch_size": 32,
                    "num_workers": 8,
                    "prefetch_factor": 4
                }
            },
            priority=100
        )
        
        # ä¸­ç­‰æ€§èƒ½é…ç½® (4-5 GPUs, 12-24GB each)
        profiles["medium_performance"] = DynamicConfigProfile(
            name="medium_performance",
            description="ä¸­ç­‰æ€§èƒ½é…ç½® - é€‚ç”¨äº4-5ä¸ª12-24GBæ˜¾å­˜çš„GPU",
            hardware_requirements={
                "min_gpu_count": 4,
                "max_gpu_count": 5,
                "min_gpu_memory_mb": 12000,
                "min_total_memory_mb": 48000
            },
            config_template={
                "stable_diffusion": {
                    "model_name": "runwayml/stable-diffusion-v1-5",
                    "device": "auto",
                    "torch_dtype": "float16",
                    "num_inference_steps": 40,
                    "guidance_scale": 7.5,
                    "height": 512,
                    "width": 512,
                    "num_images_per_prompt": 2,
                    "enable_attention_slicing": True,
                    "enable_xformers": True,
                    "enable_cpu_offload": False,
                    "enable_safety_checker": False,
                    "enable_multi_gpu": True,
                    "models_per_gpu": 1,
                    "max_models_per_gpu": 2
                },
                "multi_gpu": {
                    "batch_size_per_gpu": 4,
                    "max_workers": 8,
                    "memory_fraction": 0.85,
                    "enable_mixed_precision": True,
                    "enable_compile": True,
                    "load_balancing": True
                },
                "detection": {
                    "batch_size": 16,
                    "num_workers": 6,
                    "prefetch_factor": 3
                }
            },
            priority=80
        )
        
        # æ ‡å‡†é…ç½® (2-3 GPUs, 8-12GB each)
        profiles["standard"] = DynamicConfigProfile(
            name="standard",
            description="æ ‡å‡†é…ç½® - é€‚ç”¨äº2-3ä¸ª8-12GBæ˜¾å­˜çš„GPU",
            hardware_requirements={
                "min_gpu_count": 2,
                "max_gpu_count": 3,
                "min_gpu_memory_mb": 8000,
                "min_total_memory_mb": 16000
            },
            config_template={
                "stable_diffusion": {
                    "model_name": "runwayml/stable-diffusion-v1-5",
                    "device": "auto",
                    "torch_dtype": "float16",
                    "num_inference_steps": 30,
                    "guidance_scale": 7.5,
                    "height": 512,
                    "width": 512,
                    "num_images_per_prompt": 1,
                    "enable_attention_slicing": True,
                    "enable_xformers": True,
                    "enable_cpu_offload": True,
                    "enable_safety_checker": False,
                    "enable_multi_gpu": True,
                    "models_per_gpu": 1,
                    "max_models_per_gpu": 1
                },
                "multi_gpu": {
                    "batch_size_per_gpu": 2,
                    "max_workers": 4,
                    "memory_fraction": 0.8,
                    "enable_mixed_precision": True,
                    "enable_compile": False,
                    "load_balancing": True
                },
                "detection": {
                    "batch_size": 8,
                    "num_workers": 4,
                    "prefetch_factor": 2
                }
            },
            priority=60
        )
        
        # åŸºç¡€é…ç½® (1 GPU, 6-8GB)
        profiles["basic"] = DynamicConfigProfile(
            name="basic",
            description="åŸºç¡€é…ç½® - é€‚ç”¨äºå•ä¸ª6-8GBæ˜¾å­˜çš„GPU",
            hardware_requirements={
                "min_gpu_count": 1,
                "max_gpu_count": 1,
                "min_gpu_memory_mb": 6000,
                "min_total_memory_mb": 6000
            },
            config_template={
                "stable_diffusion": {
                    "model_name": "runwayml/stable-diffusion-v1-5",
                    "device": "auto",
                    "torch_dtype": "float16",
                    "num_inference_steps": 20,
                    "guidance_scale": 7.5,
                    "height": 512,
                    "width": 512,
                    "num_images_per_prompt": 1,
                    "enable_attention_slicing": True,
                    "enable_xformers": False,
                    "enable_cpu_offload": True,
                    "enable_safety_checker": False,
                    "enable_multi_gpu": False,
                    "models_per_gpu": 1,
                    "max_models_per_gpu": 1
                },
                "multi_gpu": {
                    "batch_size_per_gpu": 1,
                    "max_workers": 1,
                    "memory_fraction": 0.7,
                    "enable_mixed_precision": False,
                    "enable_compile": False,
                    "load_balancing": False
                },
                "detection": {
                    "batch_size": 4,
                    "num_workers": 2,
                    "prefetch_factor": 1
                }
            },
            priority=40
        )
        
        # CPUå›é€€é…ç½®
        profiles["cpu_fallback"] = DynamicConfigProfile(
            name="cpu_fallback",
            description="CPUå›é€€é…ç½® - æ— å¯ç”¨GPUæ—¶ä½¿ç”¨",
            hardware_requirements={
                "min_gpu_count": 0,
                "max_gpu_count": 0
            },
            config_template={
                "stable_diffusion": {
                    "model_name": "runwayml/stable-diffusion-v1-5",
                    "device": "cpu",
                    "torch_dtype": "float32",
                    "num_inference_steps": 10,
                    "guidance_scale": 7.5,
                    "height": 256,
                    "width": 256,
                    "num_images_per_prompt": 1,
                    "enable_attention_slicing": False,
                    "enable_xformers": False,
                    "enable_cpu_offload": False,
                    "enable_safety_checker": False,
                    "enable_multi_gpu": False,
                    "models_per_gpu": 1,
                    "max_models_per_gpu": 1
                },
                "multi_gpu": {
                    "batch_size_per_gpu": 1,
                    "max_workers": 1,
                    "memory_fraction": 0.5,
                    "enable_mixed_precision": False,
                    "enable_compile": False,
                    "load_balancing": False
                },
                "detection": {
                    "batch_size": 1,
                    "num_workers": 1,
                    "prefetch_factor": 1
                }
            },
            priority=10
        )
        
        return profiles
    
    def select_best_profile(self, hardware_config: HardwareConfig) -> DynamicConfigProfile:
        """æ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©æœ€ä½³é…ç½®æ¡£æ¡ˆ"""
        available_gpus = [gpu for gpu in hardware_config.gpus if gpu.is_available]
        gpu_count = len(available_gpus)
        
        if gpu_count == 0:
            return self.profiles["cpu_fallback"]
        
        total_gpu_memory = sum(gpu.memory_total for gpu in available_gpus)
        min_gpu_memory = min(gpu.memory_total for gpu in available_gpus)
        
        # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„é…ç½®æ¡£æ¡ˆ
        suitable_profiles = []
        
        for profile in self.profiles.values():
            req = profile.hardware_requirements
            
            # æ£€æŸ¥GPUæ•°é‡
            if "min_gpu_count" in req and gpu_count < req["min_gpu_count"]:
                continue
            if "max_gpu_count" in req and gpu_count > req["max_gpu_count"]:
                continue
            
            # æ£€æŸ¥GPUå†…å­˜
            if "min_gpu_memory_mb" in req and min_gpu_memory < req["min_gpu_memory_mb"]:
                continue
            if "min_total_memory_mb" in req and total_gpu_memory < req["min_total_memory_mb"]:
                continue
            
            suitable_profiles.append(profile)
        
        # é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„é…ç½®æ¡£æ¡ˆ
        if suitable_profiles:
            best_profile = max(suitable_profiles, key=lambda p: p.priority)
            self.logger.info(f"é€‰æ‹©é…ç½®æ¡£æ¡ˆ: {best_profile.name} ({best_profile.description})")
            return best_profile
        else:
            # å¦‚æœæ²¡æœ‰åˆé€‚çš„é…ç½®æ¡£æ¡ˆï¼Œä½¿ç”¨åŸºç¡€é…ç½®
            self.logger.warning("æœªæ‰¾åˆ°åˆé€‚çš„é…ç½®æ¡£æ¡ˆï¼Œä½¿ç”¨åŸºç¡€é…ç½®")
            return self.profiles["basic"]
    
    def generate_config(self, hardware_config: Optional[HardwareConfig] = None, 
                        force_profile: Optional[str] = None) -> Dict[str, Any]:
        """ç”ŸæˆåŠ¨æ€é…ç½®
        
        Args:
            hardware_config: HardwareConfigå¯¹è±¡æˆ–åŒ…å«ç¡¬ä»¶ä¿¡æ¯çš„å­—å…¸
            force_profile: å¼ºåˆ¶ä½¿ç”¨çš„é…ç½®æ¡£æ¡ˆåç§°
        """
        if hardware_config is None:
            hardware_config = self.hardware_detector.detect_hardware()
        
        # å¦‚æœä¼ å…¥çš„æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºHardwareConfigå¯¹è±¡
        if isinstance(hardware_config, dict):
            from .hardware_detector import GPUInfo, SystemInfo
            
            # åˆ›å»ºGPUä¿¡æ¯åˆ—è¡¨
            gpu_infos = []
            if 'gpu_info' in hardware_config:
                for gpu_data in hardware_config['gpu_info']:
                    if isinstance(gpu_data, dict):
                        gpu_info = GPUInfo(
                            id=gpu_data.get('id', 0),
                            name=gpu_data.get('name', 'Unknown'),
                            memory_total=gpu_data.get('memory_total', 0),
                            memory_free=gpu_data.get('memory_free', 0),
                            compute_capability=(7, 5),  # é»˜è®¤å€¼
                            is_available=gpu_data.get('is_available', True),
                            temperature=gpu_data.get('temperature'),
                            utilization=gpu_data.get('utilization')
                        )
                        gpu_infos.append(gpu_info)
            
            # å¦‚æœæ²¡æœ‰è¯¦ç»†GPUä¿¡æ¯ï¼Œæ ¹æ®gpu_countåˆ›å»ºåŸºæœ¬ä¿¡æ¯
            if not gpu_infos and hardware_config.get('gpu_count', 0) > 0:
                avg_memory = hardware_config.get('total_gpu_memory', 0) / hardware_config.get('gpu_count', 1)
                for i in range(hardware_config.get('gpu_count', 0)):
                    gpu_info = GPUInfo(
                        id=i,
                        name='GPU',
                        memory_total=int(avg_memory * 1024),  # è½¬æ¢ä¸ºMB
                        memory_free=int(avg_memory * 1024),
                        compute_capability=(7, 5),  # é»˜è®¤å€¼
                        is_available=True,
                        temperature=None,
                        utilization=None
                    )
                    gpu_infos.append(gpu_info)
            
            # åˆ›å»ºç³»ç»Ÿä¿¡æ¯
            system_info = SystemInfo(
                cpu_count=hardware_config.get('cpu_cores', 0),
                memory_total=hardware_config.get('system_memory', 0) * 1024,  # è½¬æ¢ä¸ºMB
                memory_available=int(hardware_config.get('system_memory', 0) * 1024 * 0.8),
                platform=hardware_config.get('platform', 'Unknown'),
                python_version='3.12',  # é»˜è®¤å€¼
                torch_version='2.0',    # é»˜è®¤å€¼
                cuda_version=hardware_config.get('cuda_version')
            )
            
            # åˆ›å»ºHardwareConfigå¯¹è±¡
            hardware_config = HardwareConfig(
                gpus=gpu_infos,
                system=system_info,
                recommended_config={}
            )
        
        # é€‰æ‹©æœ€ä½³é…ç½®æ¡£æ¡ˆ
        if force_profile and force_profile in self.profiles:
            profile = self.profiles[force_profile]
        else:
            profile = self.select_best_profile(hardware_config)
        
        # å¤åˆ¶é…ç½®æ¨¡æ¿
        config = profile.config_template.copy()
        
        # æ ¹æ®å®é™…ç¡¬ä»¶è°ƒæ•´é…ç½®
        available_gpus = [gpu for gpu in hardware_config.gpus if gpu.is_available]
        
        if available_gpus:
            # æ›´æ–°GPUç›¸å…³é…ç½®
            config["stable_diffusion"]["gpu_ids"] = [gpu.id for gpu in available_gpus]
            config["multi_gpu"]["gpu_ids"] = [gpu.id for gpu in available_gpus]
            
            # æ ¹æ®å®é™…GPUæ•°é‡è°ƒæ•´å¹¶å‘è®¾ç½®
            gpu_count = len(available_gpus)
            models_per_gpu = config["stable_diffusion"]["models_per_gpu"]
            max_concurrent = min(
                gpu_count * models_per_gpu * 2,
                config["multi_gpu"]["max_workers"]
            )
            config["stable_diffusion"]["max_concurrent_generations"] = max_concurrent
            
            # ç‰¹æ®ŠGPUä¼˜åŒ–
            gpu_names = [gpu.name for gpu in available_gpus]
            if any("A100" in name or "A200" in name for name in gpu_names):
                config["stable_diffusion"]["enable_tensor_cores"] = True
                config["stable_diffusion"]["enable_flash_attention"] = True
            elif any("RTX 4090" in name or "RTX 4080" in name for name in gpu_names):
                config["stable_diffusion"]["enable_tensor_cores"] = True
                config["stable_diffusion"]["enable_flash_attention"] = False
        
        # æ·»åŠ ç¡¬ä»¶ä¿¡æ¯åˆ°é…ç½®
        config["hardware_info"] = {
            "gpu_count": len(available_gpus),
            "gpu_names": [gpu.name for gpu in available_gpus],
            "total_gpu_memory_gb": sum(gpu.memory_total for gpu in available_gpus) / 1024,
            "system_memory_gb": hardware_config.system.memory_total / 1024,
            "cpu_count": hardware_config.system.cpu_count,
            "cuda_version": hardware_config.system.cuda_version,
            "profile_used": profile.name
        }
        
        return config
    
    def create_sd_config(self, dynamic_config: Dict[str, Any]) -> StableDiffusionConfig:
        """ä»åŠ¨æ€é…ç½®åˆ›å»ºSDé…ç½®å¯¹è±¡"""
        sd_config = dynamic_config["stable_diffusion"]
        
        return StableDiffusionConfig(
            model_name=sd_config["model_name"],
            device=sd_config["device"],
            torch_dtype=sd_config["torch_dtype"],
            num_inference_steps=sd_config["num_inference_steps"],
            guidance_scale=sd_config["guidance_scale"],
            height=sd_config["height"],
            width=sd_config["width"],
            num_images_per_prompt=sd_config["num_images_per_prompt"],
            enable_attention_slicing=sd_config["enable_attention_slicing"],
            enable_xformers=sd_config["enable_xformers"],
            enable_cpu_offload=sd_config["enable_cpu_offload"],
            enable_safety_checker=sd_config["enable_safety_checker"],
            enable_multi_gpu=sd_config["enable_multi_gpu"],
            gpu_ids=sd_config.get("gpu_ids"),
            models_per_gpu=sd_config["models_per_gpu"],
            max_models_per_gpu=sd_config["max_models_per_gpu"]
        )
    
    def create_multi_gpu_config(self, dynamic_config: Dict[str, Any]) -> MultiGPUConfig:
        """ä»åŠ¨æ€é…ç½®åˆ›å»ºå¤šGPUé…ç½®å¯¹è±¡"""
        mg_config = dynamic_config["multi_gpu"]
        
        return MultiGPUConfig(
            gpu_ids=mg_config.get("gpu_ids"),
            batch_size_per_gpu=mg_config["batch_size_per_gpu"],
            max_workers=mg_config["max_workers"],
            memory_fraction=mg_config["memory_fraction"],
            enable_mixed_precision=mg_config["enable_mixed_precision"],
            enable_compile=mg_config["enable_compile"],
            load_balancing=mg_config["load_balancing"]
        )
    
    def save_config(self, config: Dict[str, Any], filename: str = "dynamic_config.yaml") -> str:
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        # å¦‚æœfilenameæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ç›¸å¯¹äºconfig_dir
        if os.path.isabs(filename):
            config_path = Path(filename)
        else:
            config_path = self.config_dir / filename
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)
        
        self.logger.info(f"åŠ¨æ€é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
        return str(config_path)
    
    def load_config(self, filename: str = "dynamic_config.yaml") -> Optional[Dict[str, Any]]:
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        config_path = self.config_dir / filename
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            self.logger.info(f"åŠ¨æ€é…ç½®å·²ä» {config_path} åŠ è½½")
            return config
            
        except Exception as e:
            self.logger.error(f"åŠ è½½åŠ¨æ€é…ç½®å¤±è´¥: {e}")
            return None
    
    def auto_configure(self, save_config: bool = True) -> Dict[str, Any]:
        """è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶å¹¶ç”Ÿæˆé…ç½®"""
        self.logger.info("å¼€å§‹è‡ªåŠ¨é…ç½®...")
        
        # æ£€æµ‹ç¡¬ä»¶
        hardware_config = self.hardware_detector.detect_hardware()
        
        # ç”Ÿæˆé…ç½®
        dynamic_config = self.generate_config(hardware_config)
        
        # ä¿å­˜é…ç½®
        if save_config:
            self.save_config(dynamic_config)
            
            # åŒæ—¶ä¿å­˜ç¡¬ä»¶ä¿¡æ¯
            hardware_path = self.config_dir / "hardware_config.json"
            self.hardware_detector.save_config(hardware_config, str(hardware_path))
        
        self.logger.info("è‡ªåŠ¨é…ç½®å®Œæˆ")
        return dynamic_config
    
    def print_config_summary(self, config: Dict[str, Any]) -> None:
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*60)
        print("âš™ï¸  åŠ¨æ€é…ç½®æ‘˜è¦")
        print("="*60)
        
        # ç¡¬ä»¶ä¿¡æ¯
        if "hardware_info" in config:
            hw = config["hardware_info"]
            print(f"\nğŸ–¥ï¸  ç¡¬ä»¶ä¿¡æ¯:")
            print(f"   GPUæ•°é‡: {hw['gpu_count']}")
            if hw['gpu_count'] > 0:
                print(f"   GPUå‹å·: {', '.join(set(hw['gpu_names']))}")
                print(f"   æ€»GPUå†…å­˜: {hw['total_gpu_memory_gb']:.1f}GB")
            print(f"   ç³»ç»Ÿå†…å­˜: {hw['system_memory_gb']:.1f}GB")
            print(f"   CPUæ ¸å¿ƒ: {hw['cpu_count']}")
            print(f"   CUDAç‰ˆæœ¬: {hw['cuda_version'] or 'N/A'}")
            print(f"   ä½¿ç”¨é…ç½®æ¡£æ¡ˆ: {hw['profile_used']}")
        
        # SDé…ç½®
        if "stable_diffusion" in config:
            sd = config["stable_diffusion"]
            print(f"\nğŸ¨ Stable Diffusioné…ç½®:")
            print(f"   æ¨¡å‹: {sd['model_name']}")
            print(f"   è®¾å¤‡: {sd['device']}")
            print(f"   æ•°æ®ç±»å‹: {sd['torch_dtype']}")
            print(f"   æ¨ç†æ­¥æ•°: {sd['num_inference_steps']}")
            print(f"   æ¯æç¤ºå›¾åƒæ•°: {sd['num_images_per_prompt']}")
            print(f"   å¤šGPU: {'å¯ç”¨' if sd['enable_multi_gpu'] else 'ç¦ç”¨'}")
            if sd['enable_multi_gpu'] and 'gpu_ids' in sd:
                print(f"   GPU ID: {sd['gpu_ids']}")
                print(f"   æ¯GPUæ¨¡å‹æ•°: {sd['models_per_gpu']}")
        
        # å¤šGPUé…ç½®
        if "multi_gpu" in config:
            mg = config["multi_gpu"]
            print(f"\nğŸ”§ å¤šGPUé…ç½®:")
            print(f"   æ¯GPUæ‰¹å¤„ç†å¤§å°: {mg['batch_size_per_gpu']}")
            print(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: {mg['max_workers']}")
            print(f"   å†…å­˜ä½¿ç”¨æ¯”ä¾‹: {mg['memory_fraction']*100:.0f}%")
            print(f"   æ··åˆç²¾åº¦: {'å¯ç”¨' if mg['enable_mixed_precision'] else 'ç¦ç”¨'}")
            print(f"   ç¼–è¯‘ä¼˜åŒ–: {'å¯ç”¨' if mg['enable_compile'] else 'ç¦ç”¨'}")
            print(f"   è´Ÿè½½å‡è¡¡: {'å¯ç”¨' if mg['load_balancing'] else 'ç¦ç”¨'}")
        
        print("="*60 + "\n")


def auto_configure_system() -> Dict[str, Any]:
    """è‡ªåŠ¨é…ç½®ç³»ç»Ÿçš„ä¾¿æ·å‡½æ•°"""
    manager = DynamicConfigManager()
    config = manager.auto_configure()
    manager.print_config_summary(config)
    return config


def main():
    """Main entry point for console script"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Dynamic Configuration Generator")
    parser.add_argument("--hardware-info", type=str, help="ç¡¬ä»¶ä¿¡æ¯JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", type=str, help="è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--profile", choices=["high_performance", "medium", "standard", "basic", "cpu_fallback"], 
                        help="å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šé…ç½®æ¡£æ¡ˆ")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œæµ‹è¯•æ¨¡å¼")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    manager = DynamicConfigManager()
    
    if args.test:
        # æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨é¢„å®šä¹‰çš„ç¡¬ä»¶é…ç½®
        print("=== åŠ¨æ€é…ç½®ç®¡ç†æµ‹è¯• ===")
        
        test_configs = [
            {
                'gpu_count': 6,
                'total_gpu_memory': 144.0,  # 6 * 24GB RTX 4090
                'system_memory': 128.0,
                'cpu_cores': 32
            },
            {
                'gpu_count': 2,
                'total_gpu_memory': 16.0,   # 2 * 8GB RTX 3070
                'system_memory': 32.0,
                'cpu_cores': 16
            },
            {
                'gpu_count': 0,
                'total_gpu_memory': 0.0,    # CPU only
                'system_memory': 16.0,
                'cpu_cores': 8
            }
        ]
        
        for i, hardware_info in enumerate(test_configs, 1):
            print(f"\n--- æµ‹è¯•é…ç½® {i} ---")
            print(f"GPUæ•°é‡: {hardware_info['gpu_count']}")
            print(f"GPUå†…å­˜: {hardware_info['total_gpu_memory']}GB")
            print(f"ç³»ç»Ÿå†…å­˜: {hardware_info['system_memory']}GB")
            print(f"CPUæ ¸å¿ƒ: {hardware_info['cpu_cores']}")
            
            config = manager.generate_config(hardware_info, force_profile=args.profile)
            manager.print_config_summary(config)
            
            if args.output:
                config_path = f"{args.output}_test_{i}.yaml"
                manager.save_config(config, config_path)
                print(f"é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨å®é™…ç¡¬ä»¶æ£€æµ‹æˆ–æŒ‡å®šçš„ç¡¬ä»¶ä¿¡æ¯
        if args.hardware_info:
            # ä»æ–‡ä»¶åŠ è½½ç¡¬ä»¶ä¿¡æ¯
            import json
            with open(args.hardware_info, 'r', encoding='utf-8') as f:
                hardware_info = json.load(f)
        else:
            # è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶
            from .hardware_detector import HardwareDetector
            detector = HardwareDetector()
            hardware_info = detector.detect_hardware()
            print("=== ç¡¬ä»¶æ£€æµ‹ç»“æœ ===")
            detector.print_summary(hardware_info)
        
        print("\n=== ç”ŸæˆåŠ¨æ€é…ç½® ===")
        config = manager.generate_config(hardware_info, force_profile=args.profile)
        
        if args.verbose:
            manager.print_config_summary(config)
        
        # ä¿å­˜é…ç½®
        if args.output:
            manager.save_config(config, args.output)
            print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {args.output}")
        else:
            # é»˜è®¤ä¿å­˜è·¯å¾„
            output_path = "configs/dynamic/auto_generated_config.yaml"
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            manager.save_config(config, output_path)
            print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {output_path}")


if __name__ == "__main__":
    main()