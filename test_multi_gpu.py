#!/usr/bin/env python3
"""æµ‹è¯•å¤šGPUé…ç½®å’Œæ€§èƒ½

éªŒè¯6å—RTX 4090 GPUæ˜¯å¦è¢«å……åˆ†åˆ©ç”¨ã€‚
"""

import os
import sys
import time
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any
import logging
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.models.clip_model import CLIPModel, CLIPConfig
from src.models.sd_model import StableDiffusionModel, StableDiffusionConfig
from src.utils.config_loader import load_config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def check_gpu_availability():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    print("=== GPU å¯ç”¨æ€§æ£€æŸ¥ ===")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"GPU æ•°é‡: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name} ({memory_total:.1f} GB)")
    else:
        print("CUDA ä¸å¯ç”¨")
        return False
    
    return True


def get_gpu_memory_usage():
    """è·å–æ‰€æœ‰GPUçš„å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        return {}
    
    memory_info = {}
    for i in range(torch.cuda.device_count()):
        allocated = torch.cuda.memory_allocated(i) / 1024**2  # MB
        reserved = torch.cuda.memory_reserved(i) / 1024**2   # MB
        memory_info[f'GPU_{i}'] = {
            'allocated': allocated,
            'reserved': reserved
        }
    
    return memory_info


def test_clip_multi_gpu():
    """æµ‹è¯•CLIPå¤šGPUæ€§èƒ½"""
    print("\n=== CLIP å¤šGPU æµ‹è¯• ===")
    
    # åŠ è½½é…ç½®
    config = load_config()
    clip_config = CLIPConfig(**config['models']['clip'])
    
    print(f"CLIPé…ç½®: å¤šGPU={clip_config.use_multi_gpu}, GPU={clip_config.gpu_ids}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    start_time = time.time()
    clip_model = CLIPModel(clip_config)
    init_time = time.time() - start_time
    print(f"CLIPæ¨¡å‹åˆå§‹åŒ–æ—¶é—´: {init_time:.2f}ç§’")
    
    # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
    initial_memory = get_gpu_memory_usage()
    print("\nåˆå§‹GPUå†…å­˜ä½¿ç”¨:")
    for gpu, mem in initial_memory.items():
        print(f"  {gpu}: å·²åˆ†é…={mem['allocated']:.1f}MB, å·²ä¿ç•™={mem['reserved']:.1f}MB")
    
    # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
    test_texts = [
        f"A beautiful landscape with mountains and lakes {i}"
        for i in range(1000)
    ]
    
    print(f"\nå¼€å§‹ç¼–ç  {len(test_texts)} ä¸ªæ–‡æœ¬...")
    
    # æ‰¹é‡ç¼–ç æµ‹è¯•
    start_time = time.time()
    text_features = clip_model.encode_text(test_texts)
    encoding_time = time.time() - start_time
    
    print(f"æ–‡æœ¬ç¼–ç å®Œæˆ: {encoding_time:.2f}ç§’")
    print(f"ååé‡: {len(test_texts)/encoding_time:.1f} æ–‡æœ¬/ç§’")
    print(f"ç‰¹å¾å½¢çŠ¶: {text_features.shape}")
    
    # è·å–ç¼–ç åå†…å­˜ä½¿ç”¨
    final_memory = get_gpu_memory_usage()
    print("\nç¼–ç åGPUå†…å­˜ä½¿ç”¨:")
    for gpu, mem in final_memory.items():
        initial = initial_memory.get(gpu, {'allocated': 0, 'reserved': 0})
        allocated_diff = mem['allocated'] - initial['allocated']
        reserved_diff = mem['reserved'] - initial['reserved']
        print(f"  {gpu}: å·²åˆ†é…={mem['allocated']:.1f}MB (+{allocated_diff:.1f}), "
              f"å·²ä¿ç•™={mem['reserved']:.1f}MB (+{reserved_diff:.1f})")
    
    return clip_model, text_features


def test_sd_multi_gpu():
    """æµ‹è¯•Stable Diffusionå¤šGPUæ€§èƒ½"""
    print("\n=== Stable Diffusion å¤šGPU æµ‹è¯• ===")
    
    # åŠ è½½é…ç½®
    config = load_config()
    sd_config = StableDiffusionConfig(**config['models']['stable_diffusion'])
    
    print(f"SDé…ç½®: å¤šGPU={sd_config.use_multi_gpu}, GPU={sd_config.gpu_ids}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    start_time = time.time()
    sd_model = StableDiffusionModel(sd_config)
    init_time = time.time() - start_time
    print(f"SDæ¨¡å‹åˆå§‹åŒ–æ—¶é—´: {init_time:.2f}ç§’")
    
    # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
    initial_memory = get_gpu_memory_usage()
    print("\nåˆå§‹GPUå†…å­˜ä½¿ç”¨:")
    for gpu, mem in initial_memory.items():
        print(f"  {gpu}: å·²åˆ†é…={mem['allocated']:.1f}MB, å·²ä¿ç•™={mem['reserved']:.1f}MB")
    
    # ç”Ÿæˆæµ‹è¯•æç¤º
    test_prompts = [
        "A beautiful sunset over the ocean",
        "A futuristic city with flying cars",
        "A peaceful forest with sunlight filtering through trees",
        "A majestic mountain range covered in snow",
        "A colorful garden full of blooming flowers",
        "A serene lake reflecting the sky",
        "A bustling marketplace in an ancient city",
        "A cozy cabin in the woods during winter",
        "A vibrant coral reef underwater",
        "A starry night sky over a desert landscape"
    ]
    
    print(f"\nå¼€å§‹ç”Ÿæˆ {len(test_prompts)} å¼ å›¾åƒ...")
    
    # æ‰¹é‡ç”Ÿæˆæµ‹è¯•
    start_time = time.time()
    
    if sd_model.multi_gpu_manager is not None:
        # å¤šGPUå¹¶è¡Œç”Ÿæˆ
        print("ä½¿ç”¨å¤šGPUå¹¶è¡Œç”Ÿæˆ")
        images = sd_model.batch_generate_images(test_prompts, num_images_per_prompt=1)
    else:
        # å•GPUç”Ÿæˆ
        print("ä½¿ç”¨å•GPUç”Ÿæˆ")
        images = []
        for prompt in test_prompts:
            img = sd_model.generate_image(prompt, num_images=1)
            images.append(img)
    
    generation_time = time.time() - start_time
    
    print(f"å›¾åƒç”Ÿæˆå®Œæˆ: {generation_time:.2f}ç§’")
    print(f"ååé‡: {len(test_prompts)/generation_time:.2f} å›¾åƒ/ç§’")
    print(f"ç”Ÿæˆå›¾åƒæ•°é‡: {len(images)}")
    
    # è·å–ç”Ÿæˆåå†…å­˜ä½¿ç”¨
    final_memory = get_gpu_memory_usage()
    print("\nç”ŸæˆåGPUå†…å­˜ä½¿ç”¨:")
    for gpu, mem in final_memory.items():
        initial = initial_memory.get(gpu, {'allocated': 0, 'reserved': 0})
        allocated_diff = mem['allocated'] - initial['allocated']
        reserved_diff = mem['reserved'] - initial['reserved']
        print(f"  {gpu}: å·²åˆ†é…={mem['allocated']:.1f}MB (+{allocated_diff:.1f}), "
              f"å·²ä¿ç•™={mem['reserved']:.1f}MB (+{reserved_diff:.1f})")
    
    # è·å–GPUç»Ÿè®¡ä¿¡æ¯
    if hasattr(sd_model, 'get_gpu_stats'):
        gpu_stats = sd_model.get_gpu_stats()
        print("\nGPUç»Ÿè®¡ä¿¡æ¯:")
        if 'gpu_stats' in gpu_stats:
            for gpu_id, stats in gpu_stats['gpu_stats'].items():
                print(f"  GPU {gpu_id}: å·¥ä½œå™¨={stats['worker_count']}, "
                      f"å¿™ç¢Œ={stats['busy_workers']}, ç”Ÿæˆæ•°={stats['total_generations']}")
    
    return sd_model, images


def test_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\n=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
    
    # æµ‹è¯•ä¸åŒæ‰¹å¤„ç†å¤§å°çš„æ€§èƒ½
    batch_sizes = [1, 4, 8, 16, 32]
    results = []
    
    config = load_config()
    
    for batch_size in batch_sizes:
        print(f"\næµ‹è¯•æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        # ä¿®æ”¹é…ç½®
        clip_config = CLIPConfig(**config['models']['clip'])
        clip_config.batch_size = batch_size
        
        # åˆå§‹åŒ–æ¨¡å‹
        clip_model = CLIPModel(clip_config)
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_texts = [f"Test text {i}" for i in range(100)]
        
        # æµ‹è¯•ç¼–ç æ—¶é—´
        start_time = time.time()
        features = clip_model.encode_text(test_texts)
        encoding_time = time.time() - start_time
        
        throughput = len(test_texts) / encoding_time
        
        results.append({
            'batch_size': batch_size,
            'encoding_time': encoding_time,
            'throughput': throughput
        })
        
        print(f"ç¼–ç æ—¶é—´: {encoding_time:.2f}ç§’, ååé‡: {throughput:.1f} æ–‡æœ¬/ç§’")
        
        # æ¸…ç†
        del clip_model
        torch.cuda.empty_cache()
    
    return results


def visualize_results(results: List[Dict[str, Any]]):
    """å¯è§†åŒ–æµ‹è¯•ç»“æœ"""
    print("\n=== ç»“æœå¯è§†åŒ– ===")
    
    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    batch_sizes = [r['batch_size'] for r in results]
    encoding_times = [r['encoding_time'] for r in results]
    throughputs = [r['throughput'] for r in results]
    
    # ç¼–ç æ—¶é—´å›¾
    ax1.plot(batch_sizes, encoding_times, 'b-o')
    ax1.set_xlabel('æ‰¹å¤„ç†å¤§å°')
    ax1.set_ylabel('ç¼–ç æ—¶é—´ (ç§’)')
    ax1.set_title('æ‰¹å¤„ç†å¤§å° vs ç¼–ç æ—¶é—´')
    ax1.grid(True)
    
    # ååé‡å›¾
    ax2.plot(batch_sizes, throughputs, 'r-o')
    ax2.set_xlabel('æ‰¹å¤„ç†å¤§å°')
    ax2.set_ylabel('ååé‡ (æ–‡æœ¬/ç§’)')
    ax2.set_title('æ‰¹å¤„ç†å¤§å° vs ååé‡')
    ax2.grid(True)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_dir = Path('./results/multi_gpu_test')
    output_dir.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
    print(f"æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {output_dir / 'performance_comparison.png'}")
    
    plt.show()


def main():
    """ä¸»å‡½æ•°"""
    print("å¤šGPUé…ç½®å’Œæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not check_gpu_availability():
        print("GPUä¸å¯ç”¨ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    try:
        # æµ‹è¯•CLIPå¤šGPU
        clip_model, text_features = test_clip_multi_gpu()
        
        # æµ‹è¯•SDå¤šGPU
        sd_model, images = test_sd_multi_gpu()
        
        # æ€§èƒ½å¯¹æ¯”æµ‹è¯•
        results = test_performance_comparison()
        
        # å¯è§†åŒ–ç»“æœ
        visualize_results(results)
        
        print("\n=== æµ‹è¯•æ€»ç»“ ===")
        print(f"âœ“ CLIPæ¨¡å‹å¤šGPUé…ç½®: {'æˆåŠŸ' if clip_model.is_multi_gpu else 'å¤±è´¥'}")
        print(f"âœ“ SDæ¨¡å‹å¤šGPUé…ç½®: {'æˆåŠŸ' if sd_model.multi_gpu_manager is not None else 'å¤±è´¥'}")
        print(f"âœ“ æ–‡æœ¬ç‰¹å¾ç¼–ç : {text_features.shape}")
        print(f"âœ“ å›¾åƒç”Ÿæˆ: {len(images)} å¼ ")
        
        # æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨
        final_memory = get_gpu_memory_usage()
        print("\næœ€ç»ˆGPUå†…å­˜ä½¿ç”¨:")
        for gpu, mem in final_memory.items():
            print(f"  {gpu}: å·²åˆ†é…={mem['allocated']:.1f}MB, å·²ä¿ç•™={mem['reserved']:.1f}MB")
        
        print("\nğŸ‰ å¤šGPUæµ‹è¯•å®Œæˆï¼6å—RTX 4090å·²å……åˆ†åˆ©ç”¨ã€‚")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        torch.cuda.empty_cache()
        print("\nèµ„æºæ¸…ç†å®Œæˆ")


if __name__ == "__main__":
    main()