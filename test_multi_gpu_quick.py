#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¤šGPUé…ç½®éªŒè¯è„šæœ¬
å¿«é€Ÿæ£€æŸ¥å¤šGPUé…ç½®æ˜¯å¦æ­£ç¡®
"""

import sys
import os
import torch
import warnings
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')


def test_gpu_availability():
    """æµ‹è¯•GPUå¯ç”¨æ€§"""
    print("\n=== GPU å¯ç”¨æ€§æµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    return True


def test_config_loading():
    """æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½"""
    print("\n=== é…ç½®æ–‡ä»¶åŠ è½½æµ‹è¯• ===")
    
    try:
        from src.utils.config import load_config, get_config
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        load_config('./configs/default.yaml')
        print("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # è·å–æ‰€æœ‰é…ç½®
        all_config = get_config()
        print(f"âœ… é…ç½®èŠ‚æ•°é‡: {len(all_config)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹YAMLæ•°æ®ï¼ˆå¦‚æœé…ç½®ç®¡ç†å™¨ä¿å­˜äº†åŸå§‹æ•°æ®ï¼‰
        if hasattr(all_config, 'raw_config'):
            raw_config = all_config.raw_config
            if 'models' in raw_config:
                clip_config = raw_config['models'].get('clip', {})
                print(f"  CLIP å¤šGPUé…ç½®: {clip_config.get('use_multi_gpu', False)}")
                print(f"  CLIP GPU IDs: {clip_config.get('gpu_ids', [])}")
                
                sd_config = raw_config['models'].get('stable_diffusion', {})
                print(f"  SD å¤šGPUé…ç½®: {sd_config.get('use_multi_gpu', False)}")
                print(f"  SD GPU IDs: {sd_config.get('gpu_ids', [])}")
            else:
                print("âš ï¸  åŸå§‹é…ç½®ä¸­æœªæ‰¾åˆ°modelsèŠ‚")
        else:
            print("âš ï¸  é…ç½®å·²è½¬æ¢ä¸ºdataclasså¯¹è±¡ï¼Œæ— æ³•ç›´æ¥è®¿é—®åŸå§‹YAMLç»“æ„")
            print("  è¿™æ˜¯æ­£å¸¸çš„ï¼Œé…ç½®ç®¡ç†å™¨å·¥ä½œæ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False


def test_config_objects():
    """æµ‹è¯•é…ç½®å¯¹è±¡åˆ›å»º"""
    print("\n=== é…ç½®å¯¹è±¡åˆ›å»ºæµ‹è¯• ===")
    
    try:
        from src.models.clip_model import CLIPConfig
        from src.models.sd_model import StableDiffusionConfig
        
        # æµ‹è¯•CLIPé…ç½®
        clip_config = CLIPConfig(
            use_multi_gpu=True,
            gpu_ids=[0, 1, 2, 3, 4, 5],
            parallel_type="data_parallel"
        )
        print(f"âœ… CLIPé…ç½®å¯¹è±¡åˆ›å»ºæˆåŠŸ: {clip_config.use_multi_gpu}")
        
        # æµ‹è¯•SDé…ç½®
        sd_config = StableDiffusionConfig(
            use_multi_gpu=True,
            gpu_ids=[0, 1, 2, 3, 4, 5],
            max_models_per_gpu=1
        )
        print(f"âœ… SDé…ç½®å¯¹è±¡åˆ›å»ºæˆåŠŸ: {sd_config.use_multi_gpu}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®å¯¹è±¡åˆ›å»ºå¤±è´¥: {e}")
        return False


def test_multi_gpu_imports():
    """æµ‹è¯•å¤šGPUæ¨¡å—å¯¼å…¥"""
    print("\n=== å¤šGPUæ¨¡å—å¯¼å…¥æµ‹è¯• ===")
    
    try:
        # æµ‹è¯•å¤šGPUå¤„ç†å™¨å¯¼å…¥
        from src.utils.multi_gpu_processor import MultiGPUProcessor
        print("âœ… MultiGPUProcessor å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•å¤šGPU SDç®¡ç†å™¨å¯¼å…¥
        from src.models.multi_gpu_sd_manager import MultiGPUSDManager
        print("âœ… MultiGPUSDManager å¯¼å…¥æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤šGPUæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_pytorch_multi_gpu():
    """æµ‹è¯•PyTorchå¤šGPUåŸºç¡€åŠŸèƒ½"""
    print("\n=== PyTorch å¤šGPUåŸºç¡€æµ‹è¯• ===")
    
    try:
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å¤šGPUæµ‹è¯•")
            return False
        
        # æµ‹è¯•å¤šGPUå¼ é‡æ“ä½œ
        device_count = torch.cuda.device_count()
        if device_count < 2:
            print(f"âš ï¸  åªæœ‰{device_count}ä¸ªGPUï¼Œæ— æ³•æµ‹è¯•å¤šGPU")
            return True
        
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        x = torch.randn(4, 4).cuda(0)
        y = torch.randn(4, 4).cuda(1)
        
        print(f"âœ… åœ¨GPU 0å’ŒGPU 1ä¸Šåˆ›å»ºå¼ é‡æˆåŠŸ")
        print(f"  å¼ é‡xè®¾å¤‡: {x.device}")
        print(f"  å¼ é‡yè®¾å¤‡: {y.device}")
        
        # æµ‹è¯•DataParallel
        if device_count >= 2:
            model = torch.nn.Linear(4, 2)
            model = torch.nn.DataParallel(model, device_ids=[0, 1])
            model = model.cuda()
            print("âœ… DataParallel æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ PyTorchå¤šGPUæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_yaml_config_direct():
    """ç›´æ¥æµ‹è¯•YAMLé…ç½®æ–‡ä»¶å†…å®¹"""
    print("\n=== ç›´æ¥YAMLé…ç½®æµ‹è¯• ===")
    
    try:
        import yaml
        
        with open('./configs/default.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print("âœ… ç›´æ¥YAMLåŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥modelsèŠ‚
        if 'models' in config:
            print("âœ… æ‰¾åˆ°modelsé…ç½®èŠ‚")
            
            # æ£€æŸ¥CLIPé…ç½®
            if 'clip' in config['models']:
                clip_config = config['models']['clip']
                print(f"  CLIP å¤šGPU: {clip_config.get('use_multi_gpu', False)}")
                print(f"  CLIP GPU IDs: {clip_config.get('gpu_ids', [])}")
            
            # æ£€æŸ¥SDé…ç½®
            if 'stable_diffusion' in config['models']:
                sd_config = config['models']['stable_diffusion']
                print(f"  SD å¤šGPU: {sd_config.get('use_multi_gpu', False)}")
                print(f"  SD GPU IDs: {sd_config.get('gpu_ids', [])}")
        else:
            print("âŒ æœªæ‰¾åˆ°modelsé…ç½®èŠ‚")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ ç›´æ¥YAMLé…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("å¤šGPUé…ç½®å¿«é€ŸéªŒè¯è„šæœ¬")
    print("=" * 50)
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("GPUå¯ç”¨æ€§", test_gpu_availability()))
    results.append(("ç›´æ¥YAMLé…ç½®", test_yaml_config_direct()))
    results.append(("é…ç½®ç®¡ç†å™¨åŠ è½½", test_config_loading()))
    results.append(("é…ç½®å¯¹è±¡", test_config_objects()))
    results.append(("å¤šGPUæ¨¡å—å¯¼å…¥", test_multi_gpu_imports()))
    results.append(("PyTorchå¤šGPU", test_pytorch_multi_gpu()))
    
    # æ€»ç»“ç»“æœ
    print("\n=== æµ‹è¯•ç»“æœæ€»ç»“ ===")
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¤šGPUé…ç½®æ­£å¸¸")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return 1

if __name__ == "__main__":
    sys.exit(main())