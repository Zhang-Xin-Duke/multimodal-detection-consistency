#!/usr/bin/env python3
"""
ç½‘ç»œè¿æ¥æ£€æŸ¥å’Œå¤§æ¨¡å‹åŠ è½½æ—¶é—´ä¼°ç®—è„šæœ¬
"""

import time
import requests
import subprocess
import sys
import os
from pathlib import Path
import torch
from urllib.parse import urlparse

def check_internet_connection():
    """æ£€æŸ¥äº’è”ç½‘è¿æ¥çŠ¶æ€"""
    print("=== ç½‘ç»œè¿æ¥æ£€æŸ¥ ===")
    
    # æµ‹è¯•åŸºæœ¬ç½‘ç»œè¿æ¥
    test_urls = [
        "https://www.google.com",
        "https://www.baidu.com",
        "https://huggingface.co",
        "https://github.com"
    ]
    
    connection_results = {}
    
    for url in test_urls:
        try:
            start_time = time.time()
            response = requests.get(url, timeout=10)
            end_time = time.time()
            
            if response.status_code == 200:
                latency = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                connection_results[url] = {
                    'status': 'æˆåŠŸ',
                    'latency_ms': round(latency, 2),
                    'status_code': response.status_code
                }
                print(f"âœ“ {url}: è¿æ¥æˆåŠŸ (å»¶è¿Ÿ: {latency:.2f}ms)")
            else:
                connection_results[url] = {
                    'status': 'å¤±è´¥',
                    'status_code': response.status_code
                }
                print(f"âœ— {url}: HTTP {response.status_code}")
                
        except requests.exceptions.RequestException as e:
            connection_results[url] = {
                'status': 'è¿æ¥å¤±è´¥',
                'error': str(e)
            }
            print(f"âœ— {url}: è¿æ¥å¤±è´¥ - {e}")
    
    return connection_results

def test_download_speed():
    """æµ‹è¯•ä¸‹è½½é€Ÿåº¦"""
    print("\n=== ä¸‹è½½é€Ÿåº¦æµ‹è¯• ===")
    
    # æµ‹è¯•å°æ–‡ä»¶ä¸‹è½½é€Ÿåº¦
    test_file_url = "https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json"
    
    try:
        start_time = time.time()
        response = requests.get(test_file_url, timeout=30)
        end_time = time.time()
        
        if response.status_code == 200:
            file_size = len(response.content)
            download_time = end_time - start_time
            speed_kbps = (file_size / 1024) / download_time
            
            print(f"âœ“ æµ‹è¯•æ–‡ä»¶ä¸‹è½½æˆåŠŸ")
            print(f"  - æ–‡ä»¶å¤§å°: {file_size} bytes")
            print(f"  - ä¸‹è½½æ—¶é—´: {download_time:.2f} ç§’")
            print(f"  - ä¸‹è½½é€Ÿåº¦: {speed_kbps:.2f} KB/s")
            
            return speed_kbps
        else:
            print(f"âœ— ä¸‹è½½å¤±è´¥: HTTP {response.status_code}")
            return None
            
    except Exception as e:
        print(f"âœ— ä¸‹è½½é€Ÿåº¦æµ‹è¯•å¤±è´¥: {e}")
        return None

def estimate_model_loading_time():
    """ä¼°ç®—å„ç§æ¨¡å‹çš„åŠ è½½æ—¶é—´"""
    print("\n=== æ¨¡å‹åŠ è½½æ—¶é—´ä¼°ç®— ===")
    
    # æ¨¡å‹å¤§å°ä¿¡æ¯ï¼ˆè¿‘ä¼¼å€¼ï¼‰
    model_sizes = {
        "CLIP ViT-B/32": {
            "size_mb": 150,
            "description": "CLIPè§†è§‰-æ–‡æœ¬æ¨¡å‹"
        },
        "CLIP ViT-B/16": {
            "size_mb": 350,
            "description": "CLIPé«˜åˆ†è¾¨ç‡æ¨¡å‹"
        },
        "CLIP ViT-L/14": {
            "size_mb": 890,
            "description": "CLIPå¤§å‹æ¨¡å‹"
        },
        "Stable Diffusion v1.5": {
            "size_mb": 4000,
            "description": "Stable Diffusionæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹"
        },
        "Stable Diffusion XL": {
            "size_mb": 6900,
            "description": "Stable Diffusion XLé«˜è´¨é‡æ¨¡å‹"
        },
        "Qwen-7B": {
            "size_mb": 14000,
            "description": "Qwen 7Bå‚æ•°è¯­è¨€æ¨¡å‹"
        }
    }
    
    # å‡è®¾ä¸‹è½½é€Ÿåº¦ï¼ˆå¦‚æœä¹‹å‰æµ‹è¯•æˆåŠŸçš„è¯ï¼‰
    download_speed_kbps = test_download_speed()
    if download_speed_kbps is None:
        # ä½¿ç”¨ä¿å®ˆä¼°è®¡
        download_speed_kbps = 500  # 500 KB/s
        print(f"ä½¿ç”¨ä¿å®ˆä¼°è®¡ä¸‹è½½é€Ÿåº¦: {download_speed_kbps} KB/s")
    
    print(f"\nåŸºäºä¸‹è½½é€Ÿåº¦ {download_speed_kbps:.2f} KB/s çš„åŠ è½½æ—¶é—´ä¼°ç®—:")
    print("-" * 60)
    
    for model_name, info in model_sizes.items():
        size_kb = info["size_mb"] * 1024
        estimated_time_seconds = size_kb / download_speed_kbps
        estimated_time_minutes = estimated_time_seconds / 60
        
        print(f"{model_name}:")
        print(f"  - å¤§å°: {info['size_mb']} MB")
        print(f"  - æè¿°: {info['description']}")
        print(f"  - ä¼°ç®—ä¸‹è½½æ—¶é—´: {estimated_time_seconds:.1f} ç§’ ({estimated_time_minutes:.1f} åˆ†é’Ÿ)")
        print()

def check_local_cache():
    """æ£€æŸ¥æœ¬åœ°æ¨¡å‹ç¼“å­˜"""
    print("=== æœ¬åœ°æ¨¡å‹ç¼“å­˜æ£€æŸ¥ ===")
    
    # å¸¸è§çš„æ¨¡å‹ç¼“å­˜ç›®å½•
    cache_dirs = [
        os.path.expanduser("~/.cache/huggingface"),
        os.path.expanduser("~/.cache/torch"),
        os.path.expanduser("~/.cache/clip"),
        "./data/cache/models",
        "./cache"
    ]
    
    total_cache_size = 0
    
    for cache_dir in cache_dirs:
        if os.path.exists(cache_dir):
            try:
                # è®¡ç®—ç›®å½•å¤§å°
                result = subprocess.run(
                    ["du", "-sh", cache_dir], 
                    capture_output=True, 
                    text=True, 
                    timeout=30
                )
                if result.returncode == 0:
                    size_str = result.stdout.split()[0]
                    print(f"âœ“ {cache_dir}: {size_str}")
                else:
                    print(f"? {cache_dir}: æ— æ³•è®¡ç®—å¤§å°")
            except Exception as e:
                print(f"? {cache_dir}: æ£€æŸ¥å¤±è´¥ - {e}")
        else:
            print(f"âœ— {cache_dir}: ä¸å­˜åœ¨")

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜çŠ¶æ€"""
    print("\n=== GPUå†…å­˜æ£€æŸ¥ ===")
    
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU:")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory
            allocated_memory = torch.cuda.memory_allocated(i)
            cached_memory = torch.cuda.memory_reserved(i)
            
            total_gb = total_memory / (1024**3)
            allocated_gb = allocated_memory / (1024**3)
            cached_gb = cached_memory / (1024**3)
            free_gb = total_gb - allocated_gb
            
            print(f"  GPU {i}: {gpu_name}")
            print(f"    - æ€»å†…å­˜: {total_gb:.2f} GB")
            print(f"    - å·²åˆ†é…: {allocated_gb:.2f} GB")
            print(f"    - å·²ç¼“å­˜: {cached_gb:.2f} GB")
            print(f"    - å¯ç”¨: {free_gb:.2f} GB")
            print()
    else:
        print("æœªæ£€æµ‹åˆ°CUDA GPU")

def main():
    """ä¸»å‡½æ•°"""
    print("å¤šæ¨¡æ€æ£€æµ‹ç³»ç»Ÿ - ç½‘ç»œè¿æ¥å’Œæ¨¡å‹åŠ è½½æ—¶é—´æ£€æŸ¥")
    print("=" * 60)
    
    # 1. æ£€æŸ¥ç½‘ç»œè¿æ¥
    connection_results = check_internet_connection()
    
    # 2. æ£€æŸ¥æœ¬åœ°ç¼“å­˜
    check_local_cache()
    
    # 3. ä¼°ç®—æ¨¡å‹åŠ è½½æ—¶é—´
    estimate_model_loading_time()
    
    # 4. æ£€æŸ¥GPUå†…å­˜
    check_gpu_memory()
    
    # 5. æ€»ç»“å’Œå»ºè®®
    print("\n=== æ€»ç»“å’Œå»ºè®® ===")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç½‘ç»œè¿æ¥é—®é¢˜
    failed_connections = [url for url, result in connection_results.items() 
                         if result['status'] != 'æˆåŠŸ']
    
    if failed_connections:
        print("âš ï¸  ç½‘ç»œè¿æ¥é—®é¢˜:")
        for url in failed_connections:
            print(f"   - {url}: {connection_results[url]['status']}")
        print("   å»ºè®®: æ£€æŸ¥ç½‘ç»œè®¾ç½®æˆ–ä½¿ç”¨ä»£ç†")
    else:
        print("âœ“ ç½‘ç»œè¿æ¥æ­£å¸¸")
    
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    print("1. é¦–æ¬¡è¿è¡Œæ—¶ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜")
    print("2. åç»­è¿è¡Œå°†ç›´æ¥ä»ç¼“å­˜åŠ è½½ï¼Œé€Ÿåº¦æ˜¾è‘—æå‡")
    print("3. å¦‚æœç½‘ç»œè¾ƒæ…¢ï¼Œå»ºè®®åœ¨ç©ºé—²æ—¶é—´é¢„å…ˆä¸‹è½½æ¨¡å‹")
    print("4. å¯ä»¥è€ƒè™‘ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œåˆæ­¥æµ‹è¯•")
    print("5. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜å‚¨æ¨¡å‹ç¼“å­˜")
    
if __name__ == "__main__":
    main()