# 代码模块及函数清单

## 1. 项目结构

```
多模态检测一致性实验代码/
├── src/
│   ├── __init__.py
│   ├── text_augment.py          # 文本变体生成模块
│   ├── retrieval.py             # 检索接口模块
│   ├── sd_ref.py               # Stable Diffusion参考生成模块
│   ├── ref_bank.py             # 参考向量库管理模块
│   ├── detector.py             # 对抗检测模块
│   ├── pipeline.py             # 主流程管道模块
│   ├── attacks/
│   │   ├── __init__.py
│   │   ├── hubness_attack.py   # Hubness攻击实现
│   │   ├── pgd_attack.py       # PGD图像攻击实现
│   │   └── text_attack.py      # 文本攻击实现
│   ├── models/
│   │   ├── __init__.py
│   │   ├── clip_model.py       # CLIP模型封装
│   │   ├── qwen_model.py       # Qwen模型接口
│   │   ├── sd_model.py         # Stable Diffusion模型封装
│   │   └── multi_gpu_sd_manager.py  # 多GPU Stable Diffusion管理器
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── data_loader.py      # 数据加载工具
│   │   ├── metrics.py          # 评估指标计算
│   │   ├── visualization.py    # 可视化工具
│   │   ├── config.py           # 配置管理
│   │   └── multi_gpu_processor.py   # 多GPU并行处理器
│   └── evaluation/
│       ├── __init__.py
│       ├── evaluator.py        # 主评估器
│       ├── detection_eval.py   # 检测性能评估
│       └── retrieval_eval.py   # 检索性能评估
├── configs/
│   ├── default.yaml            # 默认配置
│   ├── attack_configs/         # 攻击配置
│   └── defense_configs/        # 防御配置
├── data/
│   ├── coco/                   # COCO数据集
│   ├── flickr30k/             # Flickr30k数据集
│   └── processed/             # 预处理数据
├── experiments/
│   ├── run_experiments.py      # 实验运行脚本
│   ├── ablation_studies.py     # 消融实验
│   └── adaptive_attack.py      # 自适应攻击实验
├── tests/
│   ├── test_text_augment.py    # 单元测试
│   ├── test_retrieval.py
│   ├── test_detector.py
│   └── test_pipeline.py
├── notebooks/
│   ├── data_analysis.ipynb     # 数据分析
│   ├── result_visualization.ipynb # 结果可视化
│   └── demo.ipynb             # 演示notebook
├── requirements.txt            # 依赖包列表
├── setup.py                   # 安装脚本
└── README.md                  # 项目说明
```

## 2. 核心模块详细设计

### 2.1 text_augment.py - 文本变体生成模块

```python
from typing import List, Tuple, Optional, Dict
import numpy as np
from dataclasses import dataclass

@dataclass
class TextVariantConfig:
    """文本变体生成配置"""
    num_variants: int = 10
    similarity_threshold: float = 0.85
    max_retries: int = 3
    temperature: float = 0.7
    diversity_penalty: float = 0.1

class TextAugmenter:
    """文本变体生成器"""
    
    def __init__(self, qwen_model, clip_model, config: TextVariantConfig):
        """
        初始化文本变体生成器
        
        Args:
            qwen_model: Qwen大语言模型实例
            clip_model: CLIP模型实例（用于相似度计算）
            config: 生成配置
        """
        self.qwen_model = qwen_model
        self.clip_model = clip_model
        self.config = config
    
    def generate_variants(self, query: str, num_variants: Optional[int] = None) -> List[str]:
        """
        生成文本变体
        
        Args:
            query: 原始查询文本
            num_variants: 生成变体数量，默认使用配置值
        
        Returns:
            过滤后的文本变体列表
        
        Raises:
            ValueError: 当无法生成足够高质量变体时
        """
        pass
    
    def _generate_raw_variants(self, query: str, num_variants: int) -> List[str]:
        """调用Qwen生成原始变体"""
        pass
    
    def _filter_by_similarity(self, query: str, variants: List[str]) -> List[str]:
        """基于CLIP相似度过滤变体"""
        pass
    
    def _remove_duplicates(self, variants: List[str]) -> List[str]:
        """去除重复变体"""
        pass
    
    def compute_text_similarity(self, text1: str, text2: str) -> float:
        """
        计算两个文本的CLIP嵌入相似度
        
        Args:
            text1, text2: 待比较的文本
        
        Returns:
            余弦相似度分数 [0, 1]
        """
        pass
    
    def batch_generate_variants(self, queries: List[str]) -> List[List[str]]:
        """
        批量生成文本变体
        
        Args:
            queries: 查询文本列表
        
        Returns:
            每个查询对应的变体列表
        """
        pass

def create_augmentation_prompts(query: str) -> List[str]:
    """
    创建用于Qwen的增强提示词
    
    Args:
        query: 原始查询
    
    Returns:
        提示词列表
    """
    prompts = [
        f"请为以下图像搜索查询生成5个语义相似但表达不同的变体：'{query}'",
        f"重新表述这个图像检索查询，保持核心含义：'{query}'",
        f"用不同的词汇描述同样的图像内容：'{query}'"
    ]
    return prompts
```

### 2.2 retrieval.py - 检索接口模块

```python
from typing import List, Tuple, Optional, Dict, Union
import numpy as np
import torch
from dataclasses import dataclass

@dataclass
class RetrievalConfig:
    """检索配置"""
    top_k: int = 20
    batch_size: int = 256
    similarity_metric: str = 'cosine'  # 'cosine' or 'dot_product'
    normalize_features: bool = True
    device: str = 'cuda'

class MultiModalRetriever:
    """多模态检索器"""
    
    def __init__(self, clip_model, gallery_features: np.ndarray, 
                 gallery_metadata: List[Dict], config: RetrievalConfig):
        """
        初始化检索器
        
        Args:
            clip_model: CLIP模型实例
            gallery_features: 预计算的gallery特征 (N, D)
            gallery_metadata: gallery元数据列表
            config: 检索配置
        """
        self.clip_model = clip_model
        self.gallery_features = gallery_features
        self.gallery_metadata = gallery_metadata
        self.config = config
        
        if config.normalize_features:
            self.gallery_features = self._normalize_features(self.gallery_features)
    
    def text_to_image_retrieval(self, texts: List[str], top_k: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, List[List[Dict]]]:
        """
        文本到图像检索
        
        Args:
            texts: 查询文本列表
            top_k: 返回top-k结果，默认使用配置值
        
        Returns:
            - retrieved_indices: 检索到的图像索引 (len(texts), top_k)
            - similarity_scores: 相似度分数 (len(texts), top_k)
            - retrieved_metadata: 检索到的元数据列表
        """
        pass
    
    def image_to_image_retrieval(self, query_images: np.ndarray, top_k: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, List[List[Dict]]]:
        """
        图像到图像检索
        
        Args:
            query_images: 查询图像特征 (N, D)
            top_k: 返回top-k结果
        
        Returns:
            同text_to_image_retrieval
        """
        pass
    
    def batch_retrieve_vectors(self, text_variants: List[List[str]], top_k: Optional[int] = None) -> np.ndarray:
        """
        批量检索并返回向量
        
        Args:
            text_variants: 每个查询的文本变体列表
            top_k: 每个变体检索的top-k数量
        
        Returns:
            检索到的所有向量 (total_retrieved, D)
        """
        pass
    
    def _encode_texts(self, texts: List[str]) -> np.ndarray:
        """编码文本为特征向量"""
        pass
    
    def _compute_similarity(self, query_features: np.ndarray, gallery_features: np.ndarray) -> np.ndarray:
        """计算相似度矩阵"""
        pass
    
    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        """L2归一化特征"""
        return features / np.linalg.norm(features, axis=1, keepdims=True)
    
    def update_gallery(self, new_features: np.ndarray, new_metadata: List[Dict]):
        """
        更新gallery
        
        Args:
            new_features: 新的特征向量
            new_metadata: 新的元数据
        """
        pass

class RetrievalIndex:
    """检索索引管理器"""
    
    def __init__(self, index_type: str = 'faiss'):
        """
        初始化索引
        
        Args:
            index_type: 索引类型 ('faiss', 'annoy', 'simple')
        """
        self.index_type = index_type
        self.index = None
    
    def build_index(self, features: np.ndarray):
        """构建索引"""
        pass
    
    def search(self, query_features: np.ndarray, top_k: int) -> Tuple[np.ndarray, np.ndarray]:
        """搜索最近邻"""
        pass
```

### 2.3 sd_ref.py - Stable Diffusion参考生成模块

```python
from typing import List, Optional, Dict, Tuple
import numpy as np
import torch
from PIL import Image
from dataclasses import dataclass

@dataclass
class SDGenerationConfig:
    """SD生成配置"""
    num_images_per_text: int = 3
    batch_size: int = 4
    num_inference_steps: int = 20
    guidance_scale: float = 7.5
    height: int = 512
    width: int = 512
    seed: Optional[int] = None
    device: str = 'cuda'

@dataclass
class SDOptimizationConfig:
    """SD延迟优化配置"""
    # 离线缓存策略
    enable_offline_cache: bool = True
    cache_dir: str = "./cache/sd_references"
    cache_size_limit: str = "50GB"
    precompute_common_queries: bool = True
    
    # 在线处理策略
    fast_path_enabled: bool = True
    fallback_timeout_ms: int = 100
    async_generation: bool = True
    
    # 性能优化
    fp16_inference: bool = True
    torch_compile: bool = True
    gpu_memory_fraction: float = 0.8
    
    # 质量控制
    min_text_image_similarity: float = 0.2
    max_generation_retries: int = 3

class StableDiffusionReferenceGenerator:
    """Stable Diffusion参考向量生成器"""
    
    def __init__(self, sd_pipeline, clip_model, config: SDGenerationConfig):
        """
        初始化SD参考生成器
        
        Args:
            sd_pipeline: Stable Diffusion pipeline
            clip_model: CLIP模型（用于编码生成的图像）
            config: 生成配置
        """
        self.sd_pipeline = sd_pipeline
        self.clip_model = clip_model
        self.config = config
        
        # 设置随机种子
        if config.seed is not None:
            torch.manual_seed(config.seed)
            np.random.seed(config.seed)

class OptimizedSDReferenceGenerator(StableDiffusionReferenceGenerator):
    """优化的SD参考向量生成器（支持缓存和异步处理）"""
    
    def __init__(self, sd_pipeline, clip_model, 
                 config: SDGenerationConfig, 
                 opt_config: SDOptimizationConfig):
        """
        初始化优化的SD生成器
        
        Args:
            sd_pipeline: Stable Diffusion pipeline
            clip_model: CLIP模型
            config: 基础生成配置
            opt_config: 优化配置
        """
        super().__init__(sd_pipeline, clip_model, config)
        self.opt_config = opt_config
        self.cache_manager = None
        self.async_executor = None
        
        if opt_config.enable_offline_cache:
            self._initialize_cache()
        if opt_config.async_generation:
            self._initialize_async_executor()
    
    def generate_reference_vectors_optimized(self, text_variants: List[str]) -> np.ndarray:
        """
        优化的参考向量生成（支持缓存和快速路径）
        
        Args:
            text_variants: 文本变体列表
        
        Returns:
            参考向量矩阵
        """
        if self.opt_config.fast_path_enabled:
            return self._fast_path_generation(text_variants)
        else:
            return self._full_generation(text_variants)
    
    def _fast_path_generation(self, text_variants: List[str]) -> np.ndarray:
        """
        快速路径：优先使用缓存，超时则降级
        
        Args:
            text_variants: 文本变体列表
        
        Returns:
            参考向量矩阵
        """
        pass
    
    def _full_generation(self, text_variants: List[str]) -> np.ndarray:
        """
        完整生成路径：包含SD生成和质量控制
        
        Args:
            text_variants: 文本变体列表
        
        Returns:
            参考向量矩阵
        """
        pass
    
    def _initialize_cache(self):
        """初始化缓存管理器"""
        from src.utils.cache_manager import CacheManager
        self.cache_manager = CacheManager(self.opt_config.cache_dir)
    
    def _initialize_async_executor(self):
        """初始化异步执行器"""
        import concurrent.futures
        self.async_executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
    
    def precompute_common_references(self, common_queries: List[str]):
        """
        预计算常见查询的参考向量
        
        Args:
            common_queries: 常见查询列表
        """
        pass
    
    def generate_reference_vectors(self, text_variants: List[str]) -> np.ndarray:
        """
        为文本变体生成参考向量
        
        Args:
            text_variants: 文本变体列表
        
        Returns:
            平均后的参考向量 (len(text_variants), embedding_dim)
        """
        pass
    
    def _generate_images_for_text(self, text: str, num_images: int) -> List[Image.Image]:
        """
        为单个文本生成图像
        
        Args:
            text: 输入文本
            num_images: 生成图像数量
        
        Returns:
            生成的图像列表
        """
        pass
    
    def _encode_images_to_vectors(self, images: List[Image.Image]) -> np.ndarray:
        """
        将图像编码为CLIP向量
        
        Args:
            images: 图像列表
        
        Returns:
            图像向量 (len(images), embedding_dim)
        """
        pass
    
    def _average_vectors(self, vectors: np.ndarray) -> np.ndarray:
        """
        对向量求平均
        
        Args:
            vectors: 输入向量 (N, D)
        
        Returns:
            平均向量 (1, D)
        """
        return np.mean(vectors, axis=0, keepdims=True)
    
    def batch_generate_references(self, text_variants_batch: List[List[str]]) -> List[np.ndarray]:
        """
        批量生成参考向量
        
        Args:
            text_variants_batch: 批量文本变体
        
        Returns:
            每个查询的参考向量列表
        """
        pass
    
    def save_generated_images(self, images: List[Image.Image], save_dir: str, prefix: str = 'generated'):
        """
        保存生成的图像
        
        Args:
            images: 图像列表
            save_dir: 保存目录
            prefix: 文件名前缀
        """
        pass
    
    def optimize_generation_params(self, validation_texts: List[str], 
                                 target_similarity: float = 0.8) -> Dict:
        """
        优化生成参数
        
        Args:
            validation_texts: 验证文本列表
            target_similarity: 目标相似度
        
        Returns:
            优化后的参数字典
        """
        pass

class ImageQualityFilter:
    """图像质量过滤器"""
    
    def __init__(self, clip_model):
        self.clip_model = clip_model
    
    def filter_low_quality_images(self, images: List[Image.Image], 
                                text: str, threshold: float = 0.2) -> List[Image.Image]:
        """
        过滤低质量图像
        
        Args:
            images: 图像列表
            text: 对应文本
            threshold: 质量阈值
        
        Returns:
            过滤后的图像列表
        """
        pass
    
    def compute_text_image_similarity(self, image: Image.Image, text: str) -> float:
        """
        计算文本-图像相似度
        
        Args:
            image: 输入图像
            text: 输入文本
        
        Returns:
            相似度分数
        """
        pass
```

### 2.4 ref_bank.py - 参考向量库管理模块

```python
from typing import List, Optional, Dict, Tuple, Union
import numpy as np
import pickle
from pathlib import Path
from dataclasses import dataclass

@dataclass
class ReferenceBankConfig:
    """参考向量库配置"""
    normalize_vectors: bool = True
    remove_duplicates: bool = True
    duplicate_threshold: float = 0.99
    max_bank_size: Optional[int] = None
    cache_dir: Optional[str] = None

class ReferenceVectorBank:
    """参考向量库管理器"""
    
    def __init__(self, config: ReferenceBankConfig):
        """
        初始化参考向量库
        
        Args:
            config: 配置参数
        """
        self.config = config
        self.vectors = None
        self.metadata = []
        self.vector_sources = []  # 记录向量来源（'retrieval' or 'sd'）
    
    def build_reference_bank(self, retrieved_vectors: np.ndarray, 
                           sd_vectors: np.ndarray,
                           retrieved_metadata: Optional[List[Dict]] = None,
                           sd_metadata: Optional[List[Dict]] = None) -> np.ndarray:
        """
        构建参考向量库
        
        Args:
            retrieved_vectors: 检索得到的向量 (N1, D)
            sd_vectors: SD生成的向量 (N2, D)
            retrieved_metadata: 检索向量的元数据
            sd_metadata: SD向量的元数据
        
        Returns:
            合并并处理后的参考向量库 (N, D)
        """
        pass
    
    def _merge_vectors(self, retrieved_vectors: np.ndarray, 
                      sd_vectors: np.ndarray) -> np.ndarray:
        """
        合并不同来源的向量
        
        Args:
            retrieved_vectors: 检索向量
            sd_vectors: SD生成向量
        
        Returns:
            合并后的向量矩阵
        """
        pass
    
    def _normalize_vectors(self, vectors: np.ndarray) -> np.ndarray:
        """
        L2归一化向量
        
        Args:
            vectors: 输入向量 (N, D)
        
        Returns:
            归一化后的向量 (N, D)
        """
        pass
    
    def _remove_duplicate_vectors(self, vectors: np.ndarray) -> Tuple[np.ndarray, List[int]]:
        """
        移除重复向量
        
        Args:
            vectors: 输入向量 (N, D)
        
        Returns:
            - 去重后的向量 (M, D)
            - 保留的向量索引列表
        """
        pass
    
    def _limit_bank_size(self, vectors: np.ndarray) -> np.ndarray:
        """
        限制向量库大小
        
        Args:
            vectors: 输入向量 (N, D)
        
        Returns:
            截断后的向量 (min(N, max_size), D)
        """
        pass
    
    def add_vectors(self, new_vectors: np.ndarray, 
                   source: str = 'unknown',
                   metadata: Optional[List[Dict]] = None):
        """
        向向量库添加新向量
        
        Args:
            new_vectors: 新向量 (N, D)
            source: 向量来源标识
            metadata: 向量元数据
        """
        pass
    
    def get_vectors(self) -> np.ndarray:
        """
        获取当前向量库
        
        Returns:
            向量库矩阵 (N, D)
        """
        return self.vectors
    
    def get_statistics(self) -> Dict:
        """
        获取向量库统计信息
        
        Returns:
            统计信息字典
        """
        pass
    
    def save_bank(self, filepath: str):
        """
        保存向量库到文件
        
        Args:
            filepath: 保存路径
        """
        pass
    
    def load_bank(self, filepath: str):
        """
        从文件加载向量库
        
        Args:
            filepath: 文件路径
        """
        pass
    
    def clear_bank(self):
        """清空向量库"""
        pass

class AdaptiveReferenceBank(ReferenceVectorBank):
    """自适应参考向量库"""
    
    def __init__(self, config: ReferenceBankConfig, update_strategy: str = 'sliding_window'):
        """
        初始化自适应向量库
        
        Args:
            config: 配置参数
            update_strategy: 更新策略 ('sliding_window', 'importance_sampling')
        """
        super().__init__(config)
        self.update_strategy = update_strategy
        self.usage_counts = []
        self.importance_scores = []
    
    def update_bank_adaptive(self, new_vectors: np.ndarray, 
                           performance_feedback: Optional[Dict] = None):
        """
        自适应更新向量库
        
        Args:
            new_vectors: 新向量
            performance_feedback: 性能反馈信息
        """
        pass
    
    def _compute_importance_scores(self, vectors: np.ndarray) -> np.ndarray:
        """
        计算向量重要性分数
        
        Args:
            vectors: 输入向量
        
        Returns:
            重要性分数
        """
        pass
```

### 2.5 detector.py - 对抗检测模块

```python
from typing import List, Tuple, Optional, Dict, Union, Callable
import numpy as np
from dataclasses import dataclass
from enum import Enum

class AggregationMethod(Enum):
    """相似度聚合方法"""
    MEAN = 'mean'
    MAX = 'max'
    MIN = 'min'
    MEDIAN = 'median'
    WEIGHTED_MEAN = 'weighted_mean'

class ThresholdMethod(Enum):
    """阈值计算方法"""
    STATISTICAL = 'statistical'  # μ - α*σ
    PERCENTILE = 'percentile'    # 百分位数
    ADAPTIVE = 'adaptive'        # 自适应阈值
    LEARNED = 'learned'          # 学习得到的阈值

@dataclass
class DetectorConfig:
    """检测器配置"""
    aggregation_method: AggregationMethod = AggregationMethod.MEAN
    threshold_method: ThresholdMethod = ThresholdMethod.STATISTICAL
    statistical_alpha: float = 1.5  # 统计方法的标准差系数
    percentile_value: float = 5.0   # 百分位数方法的分位点
    similarity_metric: str = 'cosine'  # 'cosine' or 'dot_product'
    normalize_scores: bool = True
    min_reference_size: int = 5

class AdversarialDetector:
    """对抗样本检测器"""
    
    def __init__(self, config: DetectorConfig):
        """
        初始化检测器
        
        Args:
            config: 检测器配置
        """
        self.config = config
        self.threshold = None
        self.clean_score_stats = None
    
    def compute_consistency_score(self, candidate_vector: np.ndarray, 
                                reference_bank: np.ndarray) -> float:
        """
        计算候选向量与参考向量库的一致性分数
        
        Args:
            candidate_vector: 候选向量 (D,) 或 (1, D)
            reference_bank: 参考向量库 (N, D)
        
        Returns:
            一致性分数 [0, 1]
        """
        pass
    
    def _compute_similarities(self, candidate_vector: np.ndarray, 
                            reference_bank: np.ndarray) -> np.ndarray:
        """
        计算候选向量与参考向量的相似度
        
        Args:
            candidate_vector: 候选向量 (D,)
            reference_bank: 参考向量库 (N, D)
        
        Returns:
            相似度数组 (N,)
        """
        pass
    
    def _aggregate_similarities(self, similarities: np.ndarray) -> float:
        """
        聚合相似度分数
        
        Args:
            similarities: 相似度数组 (N,)
        
        Returns:
            聚合后的分数
        """
        pass
    
    def detect_adversarial(self, candidate_vector: np.ndarray, 
                          reference_bank: np.ndarray,
                          threshold: Optional[float] = None) -> Tuple[bool, float, Dict]:
        """
        检测候选向量是否为对抗样本
        
        Args:
            candidate_vector: 候选向量
            reference_bank: 参考向量库
            threshold: 检测阈值，默认使用预设阈值
        
        Returns:
            - is_adversarial: 是否为对抗样本
            - consistency_score: 一致性分数
            - detection_info: 检测详细信息
        """
        pass
    
    def batch_detect(self, candidate_vectors: np.ndarray, 
                    reference_bank: np.ndarray) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:
        """
        批量检测对抗样本
        
        Args:
            candidate_vectors: 候选向量矩阵 (M, D)
            reference_bank: 参考向量库 (N, D)
        
        Returns:
            - is_adversarial_array: 检测结果数组 (M,)
            - consistency_scores: 一致性分数数组 (M,)
            - detection_info_list: 检测信息列表
        """
        pass
    
    def calibrate_threshold(self, clean_vectors: np.ndarray, 
                          reference_bank: np.ndarray,
                          method: Optional[ThresholdMethod] = None) -> float:
        """
        基于干净样本校准检测阈值
        
        Args:
            clean_vectors: 干净样本向量 (M, D)
            reference_bank: 参考向量库 (N, D)
            method: 阈值计算方法
        
        Returns:
            校准后的阈值
        """
        pass
    
    def _compute_statistical_threshold(self, scores: np.ndarray) -> float:
        """
        基于统计方法计算阈值
        
        Args:
            scores: 干净样本的一致性分数
        
        Returns:
            统计阈值
        """
        pass
    
    def _compute_percentile_threshold(self, scores: np.ndarray) -> float:
        """
        基于百分位数计算阈值
        
        Args:
            scores: 干净样本的一致性分数
        
        Returns:
            百分位数阈值
        """
        pass
    
    def update_threshold_adaptive(self, new_scores: np.ndarray, 
                                labels: np.ndarray, learning_rate: float = 0.01):
        """
        自适应更新阈值
        
        Args:
            new_scores: 新的一致性分数
            labels: 对应的标签（0: 干净, 1: 对抗）
            learning_rate: 学习率
        """
        pass
    
    def get_detection_statistics(self) -> Dict:
        """
        获取检测统计信息
        
        Returns:
            统计信息字典
        """
        pass

class AdaptiveThresholdManager:
    """自适应阈值管理器（支持漂移检测和在线更新）"""
    
    def __init__(self, initial_threshold: float, 
                 monitoring_window: int = 10000,
                 drift_threshold: float = 0.1):
        """
        初始化自适应阈值管理器
        
        Args:
            initial_threshold: 初始阈值
            monitoring_window: 监控窗口大小
            drift_threshold: 漂移检测阈值
        """
        self.current_threshold = initial_threshold
        self.monitoring_window = monitoring_window
        self.drift_threshold = drift_threshold
        self.score_history = []
        self.update_history = []
    
    def update_threshold_adaptive(self, new_scores: np.ndarray, 
                                labels: np.ndarray,
                                method: str = 'exponential_smoothing',
                                alpha: float = 0.9) -> float:
        """
        自适应更新阈值
        
        Args:
            new_scores: 新的一致性分数
            labels: 对应标签（0: 干净, 1: 对抗）
            method: 更新方法
            alpha: 平滑系数
        
        Returns:
            更新后的阈值
        """
        # 检测分布漂移
        if self._detect_distribution_drift(new_scores):
            # 重新计算阈值
            clean_scores = new_scores[labels == 0]
            if len(clean_scores) > 0:
                new_threshold = self._compute_threshold_from_clean(clean_scores)
                
                if method == 'exponential_smoothing':
                    self.current_threshold = (alpha * self.current_threshold + 
                                            (1 - alpha) * new_threshold)
                elif method == 'sliding_window':
                    self.current_threshold = new_threshold
        
        return self.current_threshold
    
    def _detect_distribution_drift(self, new_scores: np.ndarray) -> bool:
        """
        检测分布漂移
        
        Args:
            new_scores: 新的分数样本
        
        Returns:
            是否检测到漂移
        """
        if len(self.score_history) < self.monitoring_window:
            self.score_history.extend(new_scores)
            return False
        
        # 使用Kolmogorov-Smirnov检验
        from scipy.stats import ks_2samp
        historical_scores = np.array(self.score_history[-self.monitoring_window:])
        statistic, p_value = ks_2samp(historical_scores, new_scores)
        
        # 更新历史记录
        self.score_history.extend(new_scores)
        if len(self.score_history) > 2 * self.monitoring_window:
            self.score_history = self.score_history[-self.monitoring_window:]
        
        return p_value < 0.05  # 显著性水平
    
    def _compute_threshold_from_clean(self, clean_scores: np.ndarray, 
                                    method: str = 'statistical',
                                    alpha: float = 1.5) -> float:
        """
        从干净样本计算阈值
        
        Args:
            clean_scores: 干净样本分数
            method: 计算方法
            alpha: 标准差系数
        
        Returns:
            计算得到的阈值
        """
        if method == 'statistical':
            return np.mean(clean_scores) - alpha * np.std(clean_scores)
        elif method == 'percentile':
            return np.percentile(clean_scores, 5)
        else:
            raise ValueError(f"Unknown threshold method: {method}")

class EnsembleDetector:
    """集成检测器"""
    
    def __init__(self, detectors: List[AdversarialDetector], 
                 combination_method: str = 'voting'):
        """
        初始化集成检测器
        
        Args:
            detectors: 基础检测器列表
            combination_method: 结合方法 ('voting', 'averaging', 'weighted')
        """
        self.detectors = detectors
        self.combination_method = combination_method
        self.weights = np.ones(len(detectors)) / len(detectors)
    
    def detect_adversarial(self, candidate_vector: np.ndarray, 
                          reference_banks: List[np.ndarray]) -> Tuple[bool, float, Dict]:
        """
        集成检测对抗样本
        
        Args:
            candidate_vector: 候选向量
            reference_banks: 参考向量库列表
        
        Returns:
            集成检测结果
        """
        pass
    
    def update_weights(self, performance_scores: np.ndarray):
        """
        更新检测器权重
        
        Args:
            performance_scores: 各检测器的性能分数
        """
        pass
```

### 2.6 pipeline.py - 主流程管道模块

```python
from typing import List, Dict, Tuple, Optional, Union
import numpy as np
import time
from dataclasses import dataclass
from pathlib import Path

@dataclass
class PipelineConfig:
    """流水线配置"""
    # 文本变体配置
    num_text_variants: int = 10
    text_similarity_threshold: float = 0.85
    
    # 检索配置
    retrieval_top_k: int = 20
    
    # SD生成配置
    sd_images_per_variant: int = 3
    
    # 检测配置
    detection_threshold: Optional[float] = None
    
    # 性能配置
    batch_size: int = 32
    use_cache: bool = True
    cache_dir: Optional[str] = None
    
    # 日志配置
    log_level: str = 'INFO'
    save_intermediate_results: bool = False

class DefensePipeline:
    """防御流水线主类"""
    
    def __init__(self, text_augmenter, retriever, sd_generator, 
                 ref_bank_manager, detector, config: PipelineConfig):
        """
        初始化防御流水线
        
        Args:
            text_augmenter: 文本变体生成器
            retriever: 检索器
            sd_generator: SD生成器
            ref_bank_manager: 参考向量库管理器
            detector: 对抗检测器
            config: 流水线配置
        """
        self.text_augmenter = text_augmenter
        self.retriever = retriever
        self.sd_generator = sd_generator
        self.ref_bank_manager = ref_bank_manager
        self.detector = detector
        self.config = config
        
        # 性能统计
        self.timing_stats = {}
        self.cache = {} if config.use_cache else None
    
    def defend_query(self, query: str, top_k: int = 10) -> Tuple[List[Dict], Dict]:
        """
        对单个查询执行防御检测
        
        Args:
            query: 原始查询文本
            top_k: 返回的检索结果数量
        
        Returns:
            - filtered_results: 过滤后的检索结果
            - defense_info: 防御过程信息
        """
        pass
    
    def _step_generate_variants(self, query: str) -> Tuple[List[str], Dict]:
        """
        步骤A: 生成文本变体
        
        Args:
            query: 原始查询
        
        Returns:
            - text_variants: 文本变体列表
            - step_info: 步骤信息
        """
        pass
    
    def _step_retrieve_references(self, text_variants: List[str]) -> Tuple[np.ndarray, Dict]:
        """
        步骤B: 检索参考向量
        
        Args:
            text_variants: 文本变体列表
        
        Returns:
            - retrieved_vectors: 检索到的向量
            - step_info: 步骤信息
        """
        pass
    
    def _step_generate_sd_references(self, text_variants: List[str]) -> Tuple[np.ndarray, Dict]:
        """
        步骤C: 生成SD参考向量
        
        Args:
            text_variants: 文本变体列表
        
        Returns:
            - sd_vectors: SD生成的向量
            - step_info: 步骤信息
        """
        pass
    
    def _step_build_reference_bank(self, retrieved_vectors: np.ndarray, 
                                  sd_vectors: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """
        步骤D: 构建参考向量库
        
        Args:
            retrieved_vectors: 检索向量
            sd_vectors: SD向量
        
        Returns:
            - reference_bank: 参考向量库
            - step_info: 步骤信息
        """
        pass
    
    def _step_detect_and_filter(self, query: str, reference_bank: np.ndarray, 
                               top_k: int) -> Tuple[List[Dict], Dict]:
        """
        步骤E-F: 检测和过滤
        
        Args:
            query: 原始查询
            reference_bank: 参考向量库
            top_k: 返回结果数量
        
        Returns:
            - filtered_results: 过滤后的结果
            - step_info: 步骤信息
        """
        pass
    
    def batch_defend_queries(self, queries: List[str], top_k: int = 10) -> Tuple[List[List[Dict]], List[Dict]]:
        """
        批量防御查询
        
        Args:
            queries: 查询列表
            top_k: 返回结果数量
        
        Returns:
            - batch_results: 批量结果
            - batch_info: 批量信息
        """
        pass
    
    def calibrate_system(self, validation_queries: List[str], 
                        validation_labels: List[int]) -> Dict:
        """
        系统校准
        
        Args:
            validation_queries: 验证查询
            validation_labels: 验证标签（0: 干净, 1: 对抗）
        
        Returns:
            校准结果信息
        """
        pass
    
    def get_performance_stats(self) -> Dict:
        """
        获取性能统计信息
        
        Returns:
            性能统计字典
        """
        pass
    
    def save_pipeline_state(self, filepath: str):
        """
        保存流水线状态
        
        Args:
            filepath: 保存路径
        """
        pass
    
    def load_pipeline_state(self, filepath: str):
        """
        加载流水线状态
        
        Args:
            filepath: 文件路径
        """
        pass

class PipelineProfiler:
    """流水线性能分析器"""
    
    def __init__(self):
        self.timing_records = []
        self.memory_records = []
        self.gpu_records = []
    
    def profile_step(self, step_name: str, func, *args, **kwargs):
        """
        分析单个步骤的性能
        
        Args:
            step_name: 步骤名称
            func: 要执行的函数
            *args, **kwargs: 函数参数
        
        Returns:
            函数执行结果
        """
        pass
    
    def get_profiling_report(self) -> Dict:
        """
        获取性能分析报告
        
        Returns:
            分析报告字典
        """
        pass

def create_defense_pipeline(config_path: str) -> DefensePipeline:
    """
    工厂函数：创建防御流水线
    
    Args:
        config_path: 配置文件路径
    
    Returns:
        配置好的防御流水线实例
    """
    pass
```

## 3. 攻击模块设计

### 3.1 attacks/hubness_attack.py

```python
from typing import List, Tuple, Optional, Dict
import numpy as np
import torch
from dataclasses import dataclass

@dataclass
class HubnessAttackConfig:
    """Hubness攻击配置（严格遵循论文参数）"""
    # 基础设置
    random_seed: int = 42
    device: str = 'cuda'
    
    # Hubness统计参数
    top_k_hubness: int = 10
    hub_count: int = 256
    softmax_temperature: float = 0.07
    
    # 图像攻击参数（ℓ∞ PGD）
    epsilon: float = 8/255
    alpha: float = 2/255  # ε/4
    pgd_iterations: int = 10
    lambda_balance: float = 0.3
    momentum: float = 1.0  # for MI-FGSM
    
    # 目标设置
    target_query_size: int = 100
    
    # 评估参数
    batch_size: int = 256
    success_threshold: float = 0.1  # mAP降至10%以下视为成功

class HubnessAttacker:
    """Hubness攻击器（复现论文方法）"""
    
    def __init__(self, clip_model, config: HubnessAttackConfig):
        """
        初始化Hubness攻击器
        
        Args:
            clip_model: CLIP模型实例
            config: 攻击配置
        """
        self.clip_model = clip_model
        self.config = config
        
        # 设置随机种子
        torch.manual_seed(config.random_seed)
        np.random.seed(config.random_seed)

class AdaptiveHubnessAttacker(HubnessAttacker):
    """自适应Hubness攻击器（针对防御方法的白盒攻击）"""
    
    def __init__(self, clip_model, config: HubnessAttackConfig, defense_info: Dict):
        """
        初始化自适应攻击器
        
        Args:
            clip_model: CLIP模型实例
            config: 攻击配置
            defense_info: 防御方法信息（文本变体生成、SD生成、阈值等）
        """
        super().__init__(clip_model, config)
        self.defense_info = defense_info
        self.beta_values = [0.5, 1.0, 2.0]  # 一致性损失平衡系数
    
    def adaptive_attack_with_consistency(self, gallery_images: np.ndarray,
                                       target_queries: List[str],
                                       text_augmenter,
                                       sd_generator) -> Tuple[np.ndarray, Dict]:
        """
        执行自适应攻击，最大化一致性分数以绕过检测
        
        Args:
            gallery_images: gallery图像数据
            target_queries: 目标查询文本
            text_augmenter: 文本变体生成器
            sd_generator: SD生成器
        
        Returns:
            - adversarial_hub: 对抗性hub图像
            - attack_info: 攻击信息
        """
        pass
    
    def _compute_adaptive_loss(self, adv_features: torch.Tensor,
                             target_features: torch.Tensor,
                             gallery_features: torch.Tensor,
                             consistency_score: torch.Tensor,
                             beta: float = 1.0) -> torch.Tensor:
        """
        计算自适应损失函数：L = L_hub - β·consistency_score
        
        Args:
            adv_features: 对抗特征
            target_features: 目标特征
            gallery_features: gallery特征
            consistency_score: 一致性分数
            beta: 平衡系数
        
        Returns:
            自适应损失值
        """
        # 原始hubness损失
        hubness_loss = self._compute_hubness_loss(
            adv_features, target_features, gallery_features
        )
        
        # 一致性最大化项（负号表示最大化）
        consistency_penalty = -beta * consistency_score
        
        return hubness_loss + consistency_penalty
    
    def _estimate_consistency_score(self, adv_features: torch.Tensor,
                                  text_variants: List[str],
                                  sd_generator) -> torch.Tensor:
        """
        估计对抗样本的一致性分数
        
        Args:
            adv_features: 对抗特征
            text_variants: 文本变体
            sd_generator: SD生成器
        
        Returns:
            估计的一致性分数
        """
        pass
    
    def create_adversarial_hub(self, gallery_images: np.ndarray, 
                              target_queries: List[str],
                              base_image: Optional[np.ndarray] = None) -> Tuple[np.ndarray, Dict]:
        """
        创建对抗性hub
        
        Args:
            gallery_images: gallery图像数据 (N, C, H, W)
            target_queries: 目标查询文本列表
            base_image: 基础图像，如果为None则随机选择
        
        Returns:
            - adversarial_hub: 对抗性hub图像
            - attack_info: 攻击信息
        """
        pass
    
    def _compute_hubness_statistics(self, gallery_features: np.ndarray, 
                                   query_features: np.ndarray) -> Dict:
        """
        计算hubness统计信息
        
        Args:
            gallery_features: gallery特征 (N, D)
            query_features: 查询特征 (M, D)
        
        Returns:
            hubness统计信息
        """
        pass
    
    def _pgd_attack_step(self, image: torch.Tensor, 
                        target_features: torch.Tensor,
                        gallery_features: torch.Tensor) -> torch.Tensor:
        """
        执行单步PGD攻击
        
        Args:
            image: 当前图像
            target_features: 目标特征
            gallery_features: gallery特征
        
        Returns:
            更新后的图像
        """
        pass
    
    def _compute_hubness_loss(self, adv_features: torch.Tensor,
                             target_features: torch.Tensor,
                             gallery_features: torch.Tensor) -> torch.Tensor:
        """
        计算hubness损失函数
        
        Args:
            adv_features: 对抗特征
            target_features: 目标特征
            gallery_features: gallery特征
        
        Returns:
            损失值
        """
        pass
    
    def evaluate_attack_success(self, adversarial_hub: np.ndarray,
                               gallery_images: np.ndarray,
                               test_queries: List[str]) -> Dict:
        """
        评估攻击成功率
        
        Args:
            adversarial_hub: 对抗性hub
            gallery_images: gallery图像
            test_queries: 测试查询
        
        Returns:
            评估结果
        """
        pass
    
    def generate_multiple_hubs(self, gallery_images: np.ndarray,
                              target_queries_list: List[List[str]],
                              num_hubs: int = 5) -> List[Tuple[np.ndarray, Dict]]:
        """
        生成多个对抗性hub
        
        Args:
            gallery_images: gallery图像
            target_queries_list: 多组目标查询
            num_hubs: hub数量
        
        Returns:
            对抗性hub列表
        """
        pass
```

## 4. 评估模块设计

### 4.1 evaluation/data_validator.py

```python
from typing import List, Dict, Tuple, Optional, Any
import numpy as np
from dataclasses import dataclass, field
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics.pairwise import cosine_similarity

@dataclass
class DataValidationConfig:
    """数据验证配置"""
    # 数据分割验证
    train_ratio: float = 0.6
    val_ratio: float = 0.2
    test_ratio: float = 0.2
    random_seed: int = 42
    stratify: bool = True
    
    # 泄漏检测
    enable_leakage_detection: bool = True
    similarity_threshold: float = 0.95
    feature_overlap_threshold: float = 0.8
    
    # 交叉验证
    cv_folds: int = 5
    enable_temporal_split: bool = False

class DataValidator:
    """数据验证器（防止数据泄漏）"""
    
    def __init__(self, config: DataValidationConfig):
        self.config = config
        self.split_info = {}
    
    def validate_data_split(self, dataset: Dict, 
                          split_method: str = 'random') -> Dict[str, Any]:
        """
        验证数据分割的有效性
        
        Args:
            dataset: 数据集
            split_method: 分割方法 ('random', 'temporal', 'stratified')
        
        Returns:
            分割后的数据集和验证报告
        """
        if split_method == 'random':
            return self._random_split(dataset)
        elif split_method == 'temporal':
            return self._temporal_split(dataset)
        elif split_method == 'stratified':
            return self._stratified_split(dataset)
        else:
            raise ValueError(f"Unknown split method: {split_method}")
    
    def detect_data_leakage(self, train_data: Dict, 
                          test_data: Dict) -> Dict[str, Any]:
        """
        检测训练集和测试集之间的数据泄漏
        
        Args:
            train_data: 训练数据
            test_data: 测试数据
        
        Returns:
            泄漏检测报告
        """
        leakage_report = {
            'has_leakage': False,
            'similarity_leakage': [],
            'feature_leakage': [],
            'recommendations': []
        }
        
        # 检测相似样本泄漏
        similarity_leakage = self._detect_similarity_leakage(train_data, test_data)
        if similarity_leakage['count'] > 0:
            leakage_report['has_leakage'] = True
            leakage_report['similarity_leakage'] = similarity_leakage
        
        # 检测特征泄漏
        feature_leakage = self._detect_feature_leakage(train_data, test_data)
        if feature_leakage['overlap_ratio'] > self.config.feature_overlap_threshold:
            leakage_report['has_leakage'] = True
            leakage_report['feature_leakage'] = feature_leakage
        
        return leakage_report
    
    def _detect_similarity_leakage(self, train_data: Dict, 
                                 test_data: Dict) -> Dict[str, Any]:
        """
        检测基于相似度的数据泄漏
        """
        train_features = train_data.get('features', [])
        test_features = test_data.get('features', [])
        
        if len(train_features) == 0 or len(test_features) == 0:
            return {'count': 0, 'pairs': []}
        
        # 计算相似度矩阵
        similarity_matrix = cosine_similarity(test_features, train_features)
        
        # 找到高相似度对
        high_sim_pairs = []
        for i, test_sim in enumerate(similarity_matrix):
            max_sim_idx = np.argmax(test_sim)
            max_sim = test_sim[max_sim_idx]
            
            if max_sim > self.config.similarity_threshold:
                high_sim_pairs.append({
                    'test_idx': i,
                    'train_idx': max_sim_idx,
                    'similarity': max_sim
                })
        
        return {
            'count': len(high_sim_pairs),
            'pairs': high_sim_pairs,
            'threshold': self.config.similarity_threshold
        }
    
    def _detect_feature_leakage(self, train_data: Dict, 
                              test_data: Dict) -> Dict[str, Any]:
        """
        检测特征级别的数据泄漏
        """
        train_features = set(train_data.get('feature_names', []))
        test_features = set(test_data.get('feature_names', []))
        
        overlap = train_features.intersection(test_features)
        union = train_features.union(test_features)
        
        overlap_ratio = len(overlap) / len(union) if len(union) > 0 else 0
        
        return {
            'overlap_features': list(overlap),
            'overlap_ratio': overlap_ratio,
            'train_only': list(train_features - test_features),
            'test_only': list(test_features - train_features)
        }
    
    def _random_split(self, dataset: Dict) -> Dict[str, Any]:
        """
        随机分割数据集
        """
        data = dataset['data']
        labels = dataset.get('labels', None)
        
        # 首先分出测试集
        if labels is not None and self.config.stratify:
            train_val_data, test_data, train_val_labels, test_labels = train_test_split(
                data, labels, test_size=self.config.test_ratio, 
                random_state=self.config.random_seed, stratify=labels
            )
        else:
            train_val_data, test_data = train_test_split(
                data, test_size=self.config.test_ratio, 
                random_state=self.config.random_seed
            )
            train_val_labels = labels
            test_labels = None
        
        # 再分出验证集
        val_ratio_adjusted = self.config.val_ratio / (1 - self.config.test_ratio)
        if train_val_labels is not None and self.config.stratify:
            train_data, val_data, train_labels, val_labels = train_test_split(
                train_val_data, train_val_labels, test_size=val_ratio_adjusted,
                random_state=self.config.random_seed, stratify=train_val_labels
            )
        else:
            train_data, val_data = train_test_split(
                train_val_data, test_size=val_ratio_adjusted,
                random_state=self.config.random_seed
            )
            train_labels = val_labels = None
        
        return {
            'train': {'data': train_data, 'labels': train_labels},
            'val': {'data': val_data, 'labels': val_labels},
            'test': {'data': test_data, 'labels': test_labels},
            'split_info': {
                'method': 'random',
                'train_size': len(train_data),
                'val_size': len(val_data),
                'test_size': len(test_data)
            }
        }
```

### 4.2 evaluation/evaluator.py

```python
from typing import List, Dict, Tuple, Optional, Union
import numpy as np
from dataclasses import dataclass
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support

@dataclass
class EvaluationConfig:
    """评估配置"""
    # 检索评估
    recall_at_k: List[int] = None
    compute_map: bool = True
    compute_mrr: bool = True
    compute_ndcg: bool = True
    
    # 检测评估
    compute_roc_auc: bool = True
    compute_fpr_at_tpr: List[float] = None
    compute_precision_recall: bool = True
    
    # 统计显著性
    num_bootstrap_samples: int = 1000
    confidence_level: float = 0.95
    
    def __post_init__(self):
        if self.recall_at_k is None:
            self.recall_at_k = [1, 5, 10]
        if self.compute_fpr_at_tpr is None:
            self.compute_fpr_at_tpr = [0.95]

class ComprehensiveEvaluator:
    """综合评估器"""
    
    def __init__(self, config: EvaluationConfig):
        """
        初始化评估器
        
        Args:
            config: 评估配置
        """
        self.config = config
        self.results_history = []
    
    def evaluate_retrieval_performance(self, predictions: np.ndarray,
                                     ground_truth: np.ndarray,
                                     relevance_scores: Optional[np.ndarray] = None) -> Dict:
        """
        评估检索性能
        
        Args:
            predictions: 预测结果 (N, K) - 每行是top-K预测
            ground_truth: 真实标签 (N, M) - 每行是相关项目ID
            relevance_scores: 相关性分数 (N, M) - 可选
        
        Returns:
            检索性能指标字典
        """
        pass
    
    def evaluate_detection_performance(self, detection_scores: np.ndarray,
                                     true_labels: np.ndarray,
                                     threshold: Optional[float] = None) -> Dict:
        """
        评估检测性能
        
        Args:
            detection_scores: 检测分数 (N,)
            true_labels: 真实标签 (N,) - 0: 干净, 1: 对抗
            threshold: 检测阈值
        
        Returns:
            检测性能指标字典
        """
        pass
    
    def evaluate_defense_effectiveness(self, clean_results: Dict,
                                     attack_results: Dict,
                                     defense_results: Dict) -> Dict:
        """
        评估防御有效性
        
        Args:
            clean_results: 干净样本结果
            attack_results: 攻击后结果
            defense_results: 防御后结果
        
        Returns:
            防御有效性指标
        """
        pass
    
    def compute_statistical_significance(self, results_list: List[Dict],
                                       baseline_results: Dict,
                                       metric_name: str) -> Dict:
        """
        计算统计显著性
        
        Args:
            results_list: 多次实验结果列表
            baseline_results: 基线结果
            metric_name: 要比较的指标名称
        
        Returns:
            统计显著性结果
        """
        pass
    
    def generate_evaluation_report(self, all_results: Dict,
                                 save_path: Optional[str] = None) -> str:
        """
        生成评估报告
        
        Args:
            all_results: 所有评估结果
            save_path: 保存路径
        
        Returns:
            报告文本
        """
        pass

    def _compute_recall_at_k(self, predictions: np.ndarray, 
                           ground_truth: np.ndarray, k: int) -> float:
        """计算Recall@K"""
        pass
    
    def _compute_map(self, predictions: np.ndarray, 
                    ground_truth: np.ndarray) -> float:
        """计算Mean Average Precision"""
        pass
    
    def _compute_mrr(self, predictions: np.ndarray, 
                    ground_truth: np.ndarray) -> float:
        """计算Mean Reciprocal Rank"""
        pass
    
    def _compute_ndcg_at_k(self, predictions: np.ndarray,
                          relevance_scores: np.ndarray, k: int) -> float:
        """计算NDCG@K"""
        pass
    
    def _compute_fpr_at_tpr(self, scores: np.ndarray, 
                           labels: np.ndarray, target_tpr: float) -> float:
        """计算指定TPR下的FPR"""
        pass
```

### 4.3 evaluation/baseline_methods.py

```python
from typing import List, Dict, Tuple, Optional
import numpy as np
from abc import ABC, abstractmethod

class BaselineDefenseMethod(ABC):
    """基线防御方法抽象基类"""
    
    @abstractmethod
    def detect_adversarial(self, query_vector: np.ndarray, 
                          gallery_vectors: np.ndarray) -> Tuple[bool, float]:
        """检测对抗样本"""
        pass
    
    @abstractmethod
    def get_method_name(self) -> str:
        """获取方法名称"""
        pass

class StatisticalOutlierDetection(BaselineDefenseMethod):
    """统计异常值检测（基于距离分布）"""
    
    def __init__(self, contamination: float = 0.1, method: str = 'isolation_forest'):
        self.contamination = contamination
        self.method = method
        self.detector = None
    
    def fit(self, clean_vectors: np.ndarray):
        """在干净数据上训练检测器"""
        if self.method == 'isolation_forest':
            from sklearn.ensemble import IsolationForest
            self.detector = IsolationForest(contamination=self.contamination, random_state=42)
        elif self.method == 'one_class_svm':
            from sklearn.svm import OneClassSVM
            self.detector = OneClassSVM(nu=self.contamination)
        elif self.method == 'local_outlier_factor':
            from sklearn.neighbors import LocalOutlierFactor
            self.detector = LocalOutlierFactor(contamination=self.contamination, novelty=True)
        
        self.detector.fit(clean_vectors)
    
    def detect_adversarial(self, query_vector: np.ndarray, 
                          gallery_vectors: np.ndarray) -> Tuple[bool, float]:
        if self.detector is None:
            raise ValueError("Detector not fitted. Call fit() first.")
        
        # 预测异常分数
        anomaly_score = self.detector.decision_function([query_vector])[0]
        is_adversarial = self.detector.predict([query_vector])[0] == -1
        
        return is_adversarial, abs(anomaly_score)
    
    def get_method_name(self) -> str:
        return f"StatisticalOutlier_{self.method}"

class DistanceBasedDetection(BaselineDefenseMethod):
    """基于距离的检测方法"""
    
    def __init__(self, distance_metric: str = 'cosine', threshold_method: str = 'percentile'):
        self.distance_metric = distance_metric
        self.threshold_method = threshold_method
        self.threshold = None
    
    def fit(self, clean_vectors: np.ndarray, clean_queries: np.ndarray):
        """计算干净样本的距离分布"""
        from sklearn.metrics.pairwise import pairwise_distances
        
        # 计算查询到gallery的距离
        distances = pairwise_distances(clean_queries, clean_vectors, metric=self.distance_metric)
        min_distances = np.min(distances, axis=1)
        
        # 设置阈值
        if self.threshold_method == 'percentile':
            self.threshold = np.percentile(min_distances, 95)
        elif self.threshold_method == 'std':
            self.threshold = np.mean(min_distances) + 2 * np.std(min_distances)
        else:
            raise ValueError(f"Unknown threshold method: {self.threshold_method}")
    
    def detect_adversarial(self, query_vector: np.ndarray, 
                          gallery_vectors: np.ndarray) -> Tuple[bool, float]:
        from sklearn.metrics.pairwise import pairwise_distances
        
        if self.threshold is None:
            raise ValueError("Detector not fitted. Call fit() first.")
        
        # 计算到gallery的最小距离
        distances = pairwise_distances([query_vector], gallery_vectors, metric=self.distance_metric)
        min_distance = np.min(distances)
        
        is_adversarial = min_distance > self.threshold
        return is_adversarial, min_distance
    
    def get_method_name(self) -> str:
        return f"DistanceBased_{self.distance_metric}_{self.threshold_method}"

class EnsembleBaseline(BaselineDefenseMethod):
    """集成基线方法"""
    
    def __init__(self, base_methods: List[BaselineDefenseMethod], 
                 combination_method: str = 'majority_vote'):
        self.base_methods = base_methods
        self.combination_method = combination_method
    
    def fit(self, clean_vectors: np.ndarray, clean_queries: np.ndarray):
        """训练所有基础方法"""
        for method in self.base_methods:
            if hasattr(method, 'fit'):
                method.fit(clean_vectors, clean_queries)
    
    def detect_adversarial(self, query_vector: np.ndarray, 
                          gallery_vectors: np.ndarray) -> Tuple[bool, float]:
        predictions = []
        scores = []
        
        for method in self.base_methods:
            is_adv, score = method.detect_adversarial(query_vector, gallery_vectors)
            predictions.append(is_adv)
            scores.append(score)
        
        if self.combination_method == 'majority_vote':
            final_prediction = sum(predictions) > len(predictions) / 2
            final_score = np.mean(scores)
        elif self.combination_method == 'max_score':
            max_idx = np.argmax(scores)
            final_prediction = predictions[max_idx]
            final_score = scores[max_idx]
        else:
            raise ValueError(f"Unknown combination method: {self.combination_method}")
        
        return final_prediction, final_score
    
    def get_method_name(self) -> str:
        method_names = [method.get_method_name() for method in self.base_methods]
        return f"Ensemble_{self.combination_method}_{'_'.join(method_names)}"

class AdversarialTrainingBaseline(BaselineDefenseMethod):
    """对抗训练基线（使用预训练的鲁棒模型）"""
    
    def __init__(self, robust_model_path: str):
        self.robust_model_path = robust_model_path
        self.robust_model = None
        self.threshold = 0.5
    
    def load_robust_model(self):
        """加载鲁棒模型"""
        # 这里应该加载预训练的鲁棒CLIP模型
        # 实际实现中需要根据具体的模型格式来加载
        pass
    
    def detect_adversarial(self, query_vector: np.ndarray, 
                          gallery_vectors: np.ndarray) -> Tuple[bool, float]:
        # 使用鲁棒模型重新编码，比较与原始向量的差异
        # 这里是简化实现，实际需要重新编码图像
        consistency_score = np.random.random()  # 占位符
        is_adversarial = consistency_score < self.threshold
        return is_adversarial, 1 - consistency_score
    
    def get_method_name(self) -> str:
        return "AdversarialTraining"

class BaselineMethodFactory:
    """基线方法工厂"""
    
    @staticmethod
    def create_all_baselines() -> List[BaselineDefenseMethod]:
        """创建所有基线方法"""
        baselines = []
        
        # 统计异常检测方法
        for method in ['isolation_forest', 'one_class_svm', 'local_outlier_factor']:
            baselines.append(StatisticalOutlierDetection(method=method))
        
        # 距离基础检测方法
        for metric in ['cosine', 'euclidean', 'manhattan']:
            for threshold_method in ['percentile', 'std']:
                baselines.append(DistanceBasedDetection(metric, threshold_method))
        
        # 集成方法
        base_methods = [
            StatisticalOutlierDetection(method='isolation_forest'),
            DistanceBasedDetection(distance_metric='cosine')
        ]
        baselines.append(EnsembleBaseline(base_methods, 'majority_vote'))
        
        return baselines
    
    @staticmethod
    def create_baseline_by_name(name: str, **kwargs) -> BaselineDefenseMethod:
        """根据名称创建基线方法"""
        if name.startswith('statistical_'):
            method = name.split('_')[1]
            return StatisticalOutlierDetection(method=method, **kwargs)
        elif name.startswith('distance_'):
            parts = name.split('_')
            metric = parts[1]
            threshold_method = parts[2] if len(parts) > 2 else 'percentile'
            return DistanceBasedDetection(metric, threshold_method, **kwargs)
        elif name == 'adversarial_training':
            return AdversarialTrainingBaseline(**kwargs)
        else:
            raise ValueError(f"Unknown baseline method: {name}")
```

## 5. 实验运行脚本

### 5.1 experiments/run_experiments.py

```python
#!/usr/bin/env python3
"""
主实验运行脚本

使用方法:
    python run_experiments.py --config configs/default.yaml --experiment_type full
    python run_experiments.py --config configs/ablation.yaml --experiment_type ablation
"""

import argparse
import yaml
import logging
from pathlib import Path
from typing import Dict, List

def setup_logging(log_level: str = 'INFO'):
    """设置日志"""
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('experiment.log'),
            logging.StreamHandler()
        ]
    )

def load_config(config_path: str) -> Dict:
    """加载配置文件"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def run_baseline_experiment(config: Dict) -> Dict:
    """运行基线实验"""
    pass

def run_attack_experiment(config: Dict, attack_type: str) -> Dict:
    """运行攻击实验"""
    pass

def run_defense_experiment(config: Dict, attack_type: str) -> Dict:
    """运行防御实验"""
    pass

def run_full_experiment_suite(config: Dict) -> Dict:
    """
    运行完整实验套件
    
    Args:
        config: 实验配置
    
    Returns:
        实验结果字典
    """
    results = {
        'baseline': {},
        'attacks': {},
        'defenses': {},
        'comparisons': {}
    }
    
    # 1. 基线实验
    logging.info("Running baseline experiments...")
    results['baseline'] = run_baseline_experiment(config)
    
    # 2. 攻击实验
    attack_types = config['attacks']['types']
    for attack_type in attack_types:
        logging.info(f"Running {attack_type} attack experiment...")
        results['attacks'][attack_type] = run_attack_experiment(config, attack_type)
    
    # 3. 防御实验
    for attack_type in attack_types:
        logging.info(f"Running defense against {attack_type}...")
        results['defenses'][attack_type] = run_defense_experiment(config, attack_type)
    
    # 4. 性能比较
    logging.info("Computing performance comparisons...")
    results['comparisons'] = compute_performance_comparisons(results)
    
    return results

def compute_performance_comparisons(results: Dict) -> Dict:
    """计算性能比较"""
    pass

def save_results(results: Dict, output_dir: str):
    """保存实验结果"""
    pass

def main():
    parser = argparse.ArgumentParser(description='Run multi-modal retrieval defense experiments')
    parser.add_argument('--config', type=str, required=True, help='Config file path')
    parser.add_argument('--experiment_type', type=str, default='full', 
                       choices=['full', 'baseline', 'attack', 'defense', 'ablation'],
                       help='Experiment type to run')
    parser.add_argument('--output_dir', type=str, default='results', help='Output directory')
    parser.add_argument('--log_level', type=str, default='INFO', help='Logging level')
    parser.add_argument('--num_runs', type=int, default=3, help='Number of experimental runs')
    
    args = parser.parse_args()
    
    # 设置日志和输出目录
    setup_logging(args.log_level)
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # 加载配置
    config = load_config(args.config)
    
    # 运行实验
    all_results = []
    for run_id in range(args.num_runs):
        logging.info(f"Starting experimental run {run_id + 1}/{args.num_runs}")
        
        if args.experiment_type == 'full':
            results = run_full_experiment_suite(config)
        elif args.experiment_type == 'baseline':
            results = run_baseline_experiment(config)
        # ... 其他实验类型
        
        all_results.append(results)
    
    # 保存结果
    save_results(all_results, args.output_dir)
    logging.info(f"Experiments completed. Results saved to {args.output_dir}")

if __name__ == '__main__':
    main()
```

## 6. 配置文件示例

### 6.1 configs/default.yaml

```yaml
# 默认实验配置
experiment:
  name: "multimodal_defense_v1"
  description: "Text variant consistency defense against adversarial hubness"
  random_seed: 42
  num_runs: 3

# 数据集配置
datasets:
  coco:
    data_dir: "data/coco"
    split: "test"
    max_samples: 1000
  flickr30k:
    data_dir: "data/flickr30k"
    split: "test"
    max_samples: 1000

# 模型配置
models:
  clip:
    model_name: "ViT-B/32"
    device: "cuda"
    batch_size: 256
  qwen:
    model_name: "Qwen/Qwen-7B-Chat"
    api_key: "your_api_key"
    temperature: 0.7
  stable_diffusion:
    model_name: "runwayml/stable-diffusion-v1-5"
    device: "cuda"
    batch_size: 4

# 攻击配置
attacks:
  types: ["hubness", "pgd_image", "textfooler"]
  hubness:
    epsilon: 0.031  # 8/255
    alpha: 0.008    # 2/255
    iterations: 10
    lambda_balance: 0.3
    target_query_size: 100
  pgd_image:
    epsilon: 0.031
    alpha: 0.008
    iterations: 10
  textfooler:
    max_replace_ratio: 0.2
    iterations: 30

# 防御配置
defense:
  text_variants:
    num_variants: 10
    similarity_threshold: 0.85
  retrieval:
    top_k: 20
  sd_generation:
    images_per_variant: 3
    guidance_scale: 7.5
    num_inference_steps: 20
  detection:
    aggregation_method: "mean"
    threshold_method: "statistical"
    statistical_alpha: 1.5
    percentile_value: 5.0

# 评估配置
evaluation:
  metrics:
    recall_at_k: [1, 5, 10]
    compute_map: true
    compute_mrr: true
    compute_ndcg: true
    compute_roc_auc: true
    fpr_at_tpr: [0.95]
  statistical:
    num_bootstrap_samples: 1000
    confidence_level: 0.95

# 硬件配置
hardware:
  gpu_devices: [0, 1, 2, 3, 4, 5]  # 6张RTX 4090
  parallel_processing: true
  memory_optimization: true
```

## 7. 单元测试设计

### 7.1 tests/test_text_augment.py

```python
import unittest
import numpy as np
from unittest.mock import Mock, patch
from src.text_augment import TextAugmenter, TextVariantConfig

class TestTextAugmenter(unittest.TestCase):
    """文本变体生成器测试"""
    
    def setUp(self):
        """测试初始化"""
        self.mock_qwen = Mock()
        self.mock_clip = Mock()
        self.config = TextVariantConfig(num_variants=5, similarity_threshold=0.8)
        self.augmenter = TextAugmenter(self.mock_qwen, self.mock_clip, self.config)
    
    def test_generate_variants_basic(self):
        """测试基本变体生成"""
        query = "a cat sitting on a chair"
        expected_variants = [
            "a feline resting on a seat",
            "cat positioned on chair",
            "kitty sitting on furniture"
        ]
        
        # Mock Qwen响应
        self.mock_qwen.generate.return_value = expected_variants
        
        # Mock CLIP相似度计算
        self.mock_clip.encode_text.return_value = np.random.randn(4, 512)
        
        variants = self.augmenter.generate_variants(query)
        
        self.assertIsInstance(variants, list)
        self.assertGreater(len(variants), 0)
        self.assertLessEqual(len(variants), self.config.num_variants)
    
    def test_similarity_filtering(self):
        """测试相似度过滤"""
        query = "dog running in park"
        variants = [
            "dog running in park",  # 完全相同，应被过滤
            "canine jogging in garden",  # 相似但不同
            "completely different text"  # 完全不同，应被过滤
        ]
        
        # Mock相似度分数
        similarity_scores = [1.0, 0.85, 0.1]
        self.mock_clip.encode_text.return_value = np.random.randn(4, 512)
        
        with patch.object(self.augmenter, 'compute_text_similarity', 
                         side_effect=similarity_scores):
            filtered = self.augmenter._filter_by_similarity(query, variants)
        
        self.assertEqual(len(filtered), 1)  # 只有中等相似度的应该保留
        self.assertIn("canine jogging in garden", filtered)
    
    def test_batch_generation(self):
        """测试批量生成"""
        queries = ["cat on chair", "dog in park", "bird in sky"]
        
        # Mock批量响应
        self.mock_qwen.generate.return_value = ["variant1", "variant2"]
        self.mock_clip.encode_text.return_value = np.random.randn(3, 512)
        
        results = self.augmenter.batch_generate_variants(queries)
        
        self.assertEqual(len(results), len(queries))
        for variants in results:
            self.assertIsInstance(variants, list)

if __name__ == '__main__':
    unittest.main()
```

### 7.2 tests/test_detector.py

```python
import unittest
import numpy as np
from unittest.mock import Mock, patch
from src.detector import AdversarialDetector, DetectionConfig
from src.ref_bank import ReferenceBank

class TestAdversarialDetector(unittest.TestCase):
    """对抗检测器测试"""
    
    def setUp(self):
        """测试初始化"""
        self.config = DetectionConfig(
            aggregation_method="mean",
            threshold_method="statistical",
            statistical_alpha=1.5
        )
        self.detector = AdversarialDetector(self.config)
        
        # 创建模拟参考向量库
        self.ref_vectors = np.random.randn(100, 512)
        self.ref_bank = Mock()
        self.ref_bank.get_reference_vectors.return_value = self.ref_vectors
    
    def test_compute_similarity_scores(self):
        """测试相似度分数计算"""
        candidate_vector = np.random.randn(512)
        
        scores = self.detector.compute_similarity_scores(
            candidate_vector, self.ref_vectors
        )
        
        self.assertEqual(len(scores), len(self.ref_vectors))
        self.assertTrue(np.all(scores >= -1) and np.all(scores <= 1))
    
    def test_aggregate_scores_mean(self):
        """测试平均聚合方法"""
        scores = np.array([0.8, 0.6, 0.9, 0.7, 0.5])
        
        aggregated = self.detector.aggregate_scores(scores, method="mean")
        
        self.assertAlmostEqual(aggregated, np.mean(scores), places=5)
    
    def test_aggregate_scores_max(self):
        """测试最大值聚合方法"""
        scores = np.array([0.8, 0.6, 0.9, 0.7, 0.5])
        
        aggregated = self.detector.aggregate_scores(scores, method="max")
        
        self.assertAlmostEqual(aggregated, np.max(scores), places=5)
    
    def test_statistical_threshold(self):
        """测试统计阈值计算"""
        clean_scores = np.random.normal(0.7, 0.1, 1000)
        
        threshold = self.detector.compute_statistical_threshold(
            clean_scores, alpha=1.5
        )
        
        expected_threshold = np.mean(clean_scores) - 1.5 * np.std(clean_scores)
        self.assertAlmostEqual(threshold, expected_threshold, places=5)
    
    def test_percentile_threshold(self):
        """测试分位数阈值计算"""
        clean_scores = np.random.uniform(0, 1, 1000)
        
        threshold = self.detector.compute_percentile_threshold(
            clean_scores, percentile=5.0
        )
        
        expected_threshold = np.percentile(clean_scores, 5.0)
        self.assertAlmostEqual(threshold, expected_threshold, places=5)
    
    def test_detect_adversarial(self):
        """测试对抗样本检测"""
        # 正常样本（高相似度）
        normal_vector = np.random.randn(512)
        normal_vector = normal_vector / np.linalg.norm(normal_vector)
        
        # 对抗样本（低相似度）
        adversarial_vector = np.random.randn(512) * 0.1
        adversarial_vector = adversarial_vector / np.linalg.norm(adversarial_vector)
        
        # 设置阈值
        self.detector.threshold = 0.5
        
        # 测试检测结果
        normal_result = self.detector.detect_adversarial(
            normal_vector, self.ref_bank
        )
        adversarial_result = self.detector.detect_adversarial(
            adversarial_vector, self.ref_bank
        )
        
        # 正常样本应该不被检测为对抗样本
        self.assertFalse(normal_result['is_adversarial'])
        # 对抗样本应该被检测出来
        self.assertTrue(adversarial_result['is_adversarial'])
```

## 8. 工具模块 (tools/)

### 8.1 tools/visualization.py

```python
from typing import List, Dict, Tuple, Optional, Any
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd

class ExperimentVisualizer:
    """实验结果可视化工具"""
    
    def __init__(self, save_dir: str = './figures', style: str = 'seaborn'):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        plt.style.use(style)
        sns.set_palette("husl")
    
    def plot_performance_comparison(self, results: Dict[str, Dict], 
                                  metrics: List[str],
                                  save_name: str = 'performance_comparison.pdf'):
        """
        绘制性能对比图
        
        Args:
            results: {method_name: {metric: value}}
            metrics: 要比较的指标列表
            save_name: 保存文件名
        """
        fig, axes = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 4))
        if len(metrics) == 1:
            axes = [axes]
        
        for i, metric in enumerate(metrics):
            methods = list(results.keys())
            values = [results[method].get(metric, 0) for method in methods]
            
            bars = axes[i].bar(methods, values)
            axes[i].set_title(f'{metric.upper()}')
            axes[i].set_ylabel('Score')
            axes[i].tick_params(axis='x', rotation=45)
            
            # 添加数值标签
            for bar, value in zip(bars, values):
                axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.save_dir / save_name, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_attack_success_rate(self, attack_results: Dict[str, float],
                               save_name: str = 'attack_success_rate.pdf'):
        """
        绘制攻击成功率图
        """
        methods = list(attack_results.keys())
        success_rates = list(attack_results.values())
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(methods, success_rates, color='red', alpha=0.7)
        plt.title('Attack Success Rate by Method')
        plt.ylabel('Success Rate')
        plt.xticks(rotation=45)
        
        # 添加成功率阈值线
        plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='50% threshold')
        
        # 添加数值标签
        for bar, rate in zip(bars, success_rates):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                   f'{rate:.1%}', ha='center', va='bottom')
        
        plt.legend()
        plt.tight_layout()
        plt.savefig(self.save_dir / save_name, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_roc_curves(self, detection_results: Dict[str, Dict],
                       save_name: str = 'roc_curves.pdf'):
        """
        绘制ROC曲线对比
        
        Args:
            detection_results: {method: {'fpr': fpr_array, 'tpr': tpr_array, 'auc': auc_value}}
        """
        plt.figure(figsize=(8, 6))
        
        for method, result in detection_results.items():
            plt.plot(result['fpr'], result['tpr'], 
                   label=f"{method} (AUC = {result['auc']:.3f})")
        
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curves Comparison')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / save_name, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_consistency_score_distribution(self, clean_scores: np.ndarray,
                                          adv_scores: np.ndarray,
                                          threshold: float,
                                          save_name: str = 'consistency_distribution.pdf'):
        """
        绘制一致性分数分布图
        """
        plt.figure(figsize=(10, 6))
        
        # 绘制直方图
        plt.hist(clean_scores, bins=50, alpha=0.7, label='Clean Samples', 
                density=True, color='blue')
        plt.hist(adv_scores, bins=50, alpha=0.7, label='Adversarial Samples', 
                density=True, color='red')
        
        # 添加阈值线
        plt.axvline(x=threshold, color='black', linestyle='--', 
                   label=f'Threshold = {threshold:.3f}')
        
        plt.xlabel('Consistency Score')
        plt.ylabel('Density')
        plt.title('Consistency Score Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / save_name, dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_results_table(self, results: Dict[str, Dict], 
                           save_name: str = 'results_table.csv'):
        """
        创建结果表格
        """
        df = pd.DataFrame(results).T
        df.to_csv(self.save_dir / save_name, float_format='%.4f')
        return df

### 8.2 tools/logger.py

```python
import logging
import sys
from pathlib import Path
from typing import Optional
import json
from datetime import datetime

class ExperimentLogger:
    """实验日志记录器"""
    
    def __init__(self, log_dir: str = './logs', 
                 experiment_name: str = 'experiment',
                 log_level: str = 'INFO'):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.experiment_name = experiment_name
        
        # 创建日志文件名（包含时间戳）
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = self.log_dir / f"{experiment_name}_{timestamp}.log"
        
        # 配置日志格式
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # 文件处理器
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        
        # 控制台处理器
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        
        # 配置logger
        self.logger = logging.getLogger(experiment_name)
        self.logger.setLevel(getattr(logging, log_level.upper()))
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        # 实验元数据
        self.metadata = {
            'experiment_name': experiment_name,
            'start_time': datetime.now().isoformat(),
            'log_file': str(log_file)
        }
    
    def log_experiment_start(self, config: dict):
        """记录实验开始"""
        self.logger.info(f"Starting experiment: {self.experiment_name}")
        self.logger.info(f"Configuration: {json.dumps(config, indent=2)}")
        self.metadata['config'] = config
    
    def log_experiment_end(self, results: dict):
        """记录实验结束"""
        self.metadata['end_time'] = datetime.now().isoformat()
        self.metadata['results'] = results
        
        self.logger.info(f"Experiment completed: {self.experiment_name}")
        self.logger.info(f"Results: {json.dumps(results, indent=2)}")
        
        # 保存元数据
        metadata_file = self.log_dir / f"{self.experiment_name}_metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)
    
    def log_step(self, step_name: str, step_results: dict, step_time: float):
        """记录实验步骤"""
        self.logger.info(f"Step '{step_name}' completed in {step_time:.2f}s")
        self.logger.debug(f"Step results: {json.dumps(step_results, indent=2)}")
    
    def log_error(self, error_msg: str, exception: Optional[Exception] = None):
        """记录错误"""
        self.logger.error(error_msg)
        if exception:
            self.logger.exception(exception)
    
    def log_warning(self, warning_msg: str):
        """记录警告"""
        self.logger.warning(warning_msg)
    
    def log_metric(self, metric_name: str, value: float, step: Optional[int] = None):
        """记录指标"""
        if step is not None:
            self.logger.info(f"Step {step} - {metric_name}: {value:.4f}")
        else:
            self.logger.info(f"{metric_name}: {value:.4f}")

### 8.3 tools/paper_utils.py

```python
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from pathlib import Path

class PaperResultsGenerator:
    """论文结果生成器（自动化表格和图表）"""
    
    def __init__(self, output_dir: str = './paper_results'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def generate_main_results_table(self, results: Dict[str, Dict[str, float]],
                                  metrics: List[str] = ['recall@1', 'recall@5', 'map'],
                                  caption: str = "Main experimental results") -> str:
        """
        生成主要结果表格（LaTeX格式）
        
        Args:
            results: {method_name: {metric: value}}
            metrics: 要包含的指标
            caption: 表格标题
        
        Returns:
            LaTeX表格代码
        """
        # 创建DataFrame
        df = pd.DataFrame(results).T
        df = df[metrics]  # 只保留指定指标
        
        # 格式化数值（保留3位小数）
        df = df.round(3)
        
        # 找到每列的最佳值并加粗
        latex_table = "\\begin{table}[htbp]\n"
        latex_table += "\\centering\n"
        latex_table += f"\\caption{{{caption}}}\n"
        latex_table += "\\label{tab:main_results}\n"
        
        # 表格头部
        num_cols = len(metrics) + 1
        latex_table += f"\\begin{{tabular}}{{l{'c' * len(metrics)}}}\n"
        latex_table += "\\toprule\n"
        
        # 列标题
        header = "Method & " + " & ".join([m.replace('_', '\\_') for m in metrics]) + " \\\\\n"
        latex_table += header
        latex_table += "\\midrule\n"
        
        # 数据行
        for method in df.index:
            row_data = []
            for metric in metrics:
                value = df.loc[method, metric]
                # 检查是否为最佳值
                if value == df[metric].max():
                    row_data.append(f"\\textbf{{{value:.3f}}}")
                else:
                    row_data.append(f"{value:.3f}")
            
            method_name = method.replace('_', '\\_')
            row = f"{method_name} & " + " & ".join(row_data) + " \\\\\n"
            latex_table += row
        
        latex_table += "\\bottomrule\n"
        latex_table += "\\end{tabular}\n"
        latex_table += "\\end{table}\n"
        
        # 保存到文件
        with open(self.output_dir / 'main_results_table.tex', 'w') as f:
            f.write(latex_table)
        
        return latex_table
    
    def generate_statistical_significance_table(self, significance_results: Dict[str, Dict],
                                              caption: str = "Statistical significance test results") -> str:
        """
        生成统计显著性检验表格
        
        Args:
            significance_results: {comparison: {'p_value': float, 'significant': bool}}
            caption: 表格标题
        
        Returns:
            LaTeX表格代码
        """
        latex_table = "\\begin{table}[htbp]\n"
        latex_table += "\\centering\n"
        latex_table += f"\\caption{{{caption}}}\n"
        latex_table += "\\label{tab:significance}\n"
        latex_table += "\\begin{tabular}{lcc}\n"
        latex_table += "\\toprule\n"
        latex_table += "Comparison & p-value & Significant \\\\\n"
        latex_table += "\\midrule\n"
        
        for comparison, result in significance_results.items():
            p_value = result['p_value']
            significant = "Yes" if result['significant'] else "No"
            
            comparison_name = comparison.replace('_', '\\_')
            if p_value < 0.001:
                p_str = "< 0.001"
            else:
                p_str = f"{p_value:.3f}"
            
            latex_table += f"{comparison_name} & {p_str} & {significant} \\\\\n"
        
        latex_table += "\\bottomrule\n"
        latex_table += "\\end{tabular}\n"
        latex_table += "\\end{table}\n"
        
        # 保存到文件
         with open(self.output_dir / 'significance_table.tex', 'w') as f:
             f.write(latex_table)
         
         return latex_table
    
    def generate_ablation_study_table(self, ablation_results: Dict[str, Dict[str, float]],
                                     baseline_method: str,
                                     caption: str = "Ablation study results") -> str:
        """
        生成消融实验表格
        
        Args:
            ablation_results: {variant_name: {metric: value}}
            baseline_method: 基线方法名称
            caption: 表格标题
        
        Returns:
            LaTeX表格代码
        """
        # 确保基线方法在第一行
        methods = [baseline_method] + [m for m in ablation_results.keys() if m != baseline_method]
        metrics = list(next(iter(ablation_results.values())).keys())
        
        latex_table = "\\begin{table}[htbp]\n"
        latex_table += "\\centering\n"
        latex_table += f"\\caption{{{caption}}}\n"
        latex_table += "\\label{tab:ablation}\n"
        latex_table += f"\\begin{{tabular}}{{l{'c' * len(metrics)}}}\n"
        latex_table += "\\toprule\n"
        
        # 列标题
        header = "Method & " + " & ".join([m.replace('_', '\\_') for m in metrics]) + " \\\\\n"
        latex_table += header
        latex_table += "\\midrule\n"
        
        # 数据行
        for i, method in enumerate(methods):
            if method not in ablation_results:
                continue
                
            row_data = []
            for metric in metrics:
                value = ablation_results[method][metric]
                if i == 0:  # 基线方法加粗
                    row_data.append(f"\\textbf{{{value:.3f}}}")
                else:
                    # 计算相对于基线的变化
                    baseline_value = ablation_results[baseline_method][metric]
                    if value > baseline_value:
                        row_data.append(f"{value:.3f} (\\textcolor{{green}}{{+{value-baseline_value:.3f}}})")
                    elif value < baseline_value:
                        row_data.append(f"{value:.3f} (\\textcolor{{red}}{{{value-baseline_value:.3f}}})")
                    else:
                        row_data.append(f"{value:.3f}")
            
            method_name = method.replace('_', '\\_')
            row = f"{method_name} & " + " & ".join(row_data) + " \\\\\n"
            latex_table += row
        
        latex_table += "\\bottomrule\n"
        latex_table += "\\end{tabular}\n"
        latex_table += "\\end{table}\n"
        
        # 保存到文件
        with open(self.output_dir / 'ablation_table.tex', 'w') as f:
            f.write(latex_table)
        
        return latex_table
    
    def generate_figure_code(self, figure_type: str, data_file: str = None) -> str:
        """
        生成图表代码（TikZ/PGFPlots格式）
        
        Args:
            figure_type: 图表类型 ('roc', 'bar', 'line', 'distribution')
            data_file: 数据文件路径
        
        Returns:
            LaTeX图表代码
        """
        if figure_type == 'roc':
            figure_code = self._generate_roc_figure_code(data_file)
        elif figure_type == 'bar':
            figure_code = self._generate_bar_figure_code(data_file)
        elif figure_type == 'line':
            figure_code = self._generate_line_figure_code(data_file)
        elif figure_type == 'distribution':
            figure_code = self._generate_distribution_figure_code(data_file)
        else:
            raise ValueError(f"Unsupported figure type: {figure_type}")
        
        # 保存到文件
        with open(self.output_dir / f'{figure_type}_figure.tex', 'w') as f:
            f.write(figure_code)
        
        return figure_code
    
    def _generate_roc_figure_code(self, data_file: str) -> str:
        """生成ROC曲线图代码"""
        return f"""
\\begin{{figure}}[htbp]
\\centering
\\begin{{tikzpicture}}
\\begin{{axis}}[
    xlabel={{False Positive Rate}},
    ylabel={{True Positive Rate}},
    legend pos=south east,
    grid=major,
    width=10cm,
    height=8cm
]

% 添加数据文件: {data_file or 'roc_data.dat'}
\\addplot[blue, thick] table[x=fpr, y=tpr] {{{data_file or 'roc_data.dat'}}};
\\addlegendentry{{Our Method}}

% 随机分类器基线
\\addplot[black, dashed] coordinates {{(0,0) (1,1)}};
\\addlegendentry{{Random}}

\\end{{axis}}
\\end{{tikzpicture}}
\\caption{{ROC curves comparison}}
\\label{{fig:roc}}
\\end{{figure}}
"""
    
    def _generate_bar_figure_code(self, data_file: str) -> str:
        """生成柱状图代码"""
        return f"""
\\begin{{figure}}[htbp]
\\centering
\\begin{{tikzpicture}}
\\begin{{axis}}[
    ybar,
    xlabel={{Methods}},
    ylabel={{Performance}},
    symbolic x coords={{Method1, Method2, Method3, Ours}},
    xtick=data,
    x tick label style={{rotate=45, anchor=east}},
    width=12cm,
    height=8cm,
    legend pos=north west
]

% 添加数据文件: {data_file or 'bar_data.dat'}
\\addplot table[x=method, y=recall] {{{data_file or 'bar_data.dat'}}};
\\addlegendentry{{Recall@1}}

\\addplot table[x=method, y=map] {{{data_file or 'bar_data.dat'}}};
\\addlegendentry{{mAP}}

\\end{{axis}}
\\end{{tikzpicture}}
\\caption{{Performance comparison}}
\\label{{fig:performance}}
\\end{{figure}}
"""
    
    def _generate_line_figure_code(self, data_file: str) -> str:
        """生成折线图代码"""
        return f"""
\\begin{{figure}}[htbp]
\\centering
\\begin{{tikzpicture}}
\\begin{{axis}}[
    xlabel={{Parameter Value}},
    ylabel={{Performance}},
    legend pos=north east,
    grid=major,
    width=10cm,
    height=8cm
]

% 添加数据文件: {data_file or 'line_data.dat'}
\\addplot[blue, thick, mark=*] table[x=param, y=performance] {{{data_file or 'line_data.dat'}}};
\\addlegendentry{{Our Method}}

\\end{{axis}}
\\end{{tikzpicture}}
\\caption{{Parameter sensitivity analysis}}
\\label{{fig:sensitivity}}
\\end{{figure}}
"""
    
    def _generate_distribution_figure_code(self, data_file: str) -> str:
        """生成分布图代码"""
        return f"""
\\begin{{figure}}[htbp]
\\centering
\\begin{{tikzpicture}}
\\begin{{axis}}[
    xlabel={{Consistency Score}},
    ylabel={{Density}},
    legend pos=north east,
    width=10cm,
    height=8cm,
    domain=0:1
]

% 添加数据文件: {data_file or 'distribution_data.dat'}
\\addplot[blue, thick, fill=blue, fill opacity=0.3] table[x=score, y=clean_density] {{{data_file or 'distribution_data.dat'}}};
\\addlegendentry{{Clean Samples}}

\\addplot[red, thick, fill=red, fill opacity=0.3] table[x=score, y=adv_density] {{{data_file or 'distribution_data.dat'}}};
\\addlegendentry{{Adversarial Samples}}

% 阈值线
\\addplot[black, dashed, thick] coordinates {{(0.5,0) (0.5,5)}};
\\addlegendentry{{Threshold}}

\\end{{axis}}
\\end{{tikzpicture}}
\\caption{{Consistency score distribution}}
\\label{{fig:distribution}}
\\end{{figure}}
"""
```

## 9. 性能优化建议

### 9.1 多GPU并行处理

```python
# utils/multi_gpu.py
import torch
import torch.nn as nn
from torch.nn.parallel import DataParallel, DistributedDataParallel
from typing import List, Dict, Any
import threading
from concurrent.futures import ThreadPoolExecutor

class MultiGPUProcessor:
    """多GPU并行处理器（特别针对Stable Diffusion生成）"""
    
    def __init__(self, device_ids: List[int] = None, batch_size_per_gpu: int = 4):
        self.device_ids = device_ids or list(range(torch.cuda.device_count()))
        self.batch_size_per_gpu = batch_size_per_gpu
        self.models = {}
        
    def setup_model_parallel(self, model_name: str, model_class, model_kwargs: dict):
        """设置模型并行"""
        if len(self.device_ids) > 1:
            # 多GPU并行
            model = model_class(**model_kwargs)
            if torch.cuda.device_count() > 1:
                model = DataParallel(model, device_ids=self.device_ids)
            self.models[model_name] = model.cuda()
        else:
            # 单GPU
            model = model_class(**model_kwargs)
            self.models[model_name] = model.cuda(self.device_ids[0])
    
    def parallel_sd_generation(self, text_queries: List[str], 
                             model_name: str = 'stable_diffusion') -> List[torch.Tensor]:
        """并行Stable Diffusion生成"""
        model = self.models[model_name]
        total_batch_size = len(self.device_ids) * self.batch_size_per_gpu
        
        results = []
        for i in range(0, len(text_queries), total_batch_size):
            batch_queries = text_queries[i:i + total_batch_size]
            
            # 分配到不同GPU
            gpu_batches = []
            for gpu_id in self.device_ids:
                start_idx = gpu_id * self.batch_size_per_gpu
                end_idx = min(start_idx + self.batch_size_per_gpu, len(batch_queries))
                if start_idx < len(batch_queries):
                    gpu_batches.append(batch_queries[start_idx:end_idx])
            
            # 并行处理
            with ThreadPoolExecutor(max_workers=len(self.device_ids)) as executor:
                futures = []
                for gpu_id, gpu_batch in zip(self.device_ids, gpu_batches):
                    future = executor.submit(self._process_on_gpu, gpu_batch, model, gpu_id)
                    futures.append(future)
                
                # 收集结果
                for future in futures:
                    results.extend(future.result())
        
        return results
    
    def _process_on_gpu(self, queries: List[str], model, gpu_id: int) -> List[torch.Tensor]:
        """在指定GPU上处理"""
        with torch.cuda.device(gpu_id):
            # 这里应该是实际的SD生成逻辑
            # 示例代码
            results = []
            for query in queries:
                # 模拟SD生成
                result = model(query)  # 实际应该是SD pipeline调用
                results.append(result)
            return results

### 9.2 缓存管理

```python
# utils/cache_manager.py
import pickle
import hashlib
from pathlib import Path
from typing import Any, Optional, Dict
import torch
import numpy as np
from collections import OrderedDict
import threading

class CacheManager:
    """缓存管理器（用于缓存文本变体和SD向量）"""
    
    def __init__(self, cache_dir: str = './cache', 
                 max_memory_size: int = 1000,  # 内存中最大缓存项数
                 max_disk_size: int = 10000):   # 磁盘最大缓存项数
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.max_memory_size = max_memory_size
        self.max_disk_size = max_disk_size
        
        # 内存缓存（LRU）
        self.memory_cache = OrderedDict()
        self.cache_lock = threading.RLock()
        
        # 磁盘缓存索引
        self.disk_index_file = self.cache_dir / 'cache_index.pkl'
        self.disk_index = self._load_disk_index()
    
    def _generate_key(self, data: Any) -> str:
        """生成缓存键"""
        if isinstance(data, str):
            content = data.encode('utf-8')
        elif isinstance(data, (list, tuple)):
            content = str(data).encode('utf-8')
        elif isinstance(data, dict):
            content = str(sorted(data.items())).encode('utf-8')
        else:
            content = str(data).encode('utf-8')
        
        return hashlib.md5(content).hexdigest()
    
    def get(self, key: Any) -> Optional[Any]:
        """获取缓存项"""
        cache_key = self._generate_key(key)
        
        with self.cache_lock:
            # 首先检查内存缓存
            if cache_key in self.memory_cache:
                # 移到最后（LRU更新）
                value = self.memory_cache.pop(cache_key)
                self.memory_cache[cache_key] = value
                return value
            
            # 检查磁盘缓存
            if cache_key in self.disk_index:
                file_path = self.cache_dir / f"{cache_key}.pkl"
                if file_path.exists():
                    try:
                        with open(file_path, 'rb') as f:
                            value = pickle.load(f)
                        
                        # 加载到内存缓存
                        self._add_to_memory_cache(cache_key, value)
                        return value
                    except Exception:
                        # 文件损坏，删除
                        file_path.unlink(missing_ok=True)
                        del self.disk_index[cache_key]
        
        return None
    
    def put(self, key: Any, value: Any, force_disk: bool = False):
        """存储缓存项"""
        cache_key = self._generate_key(key)
        
        with self.cache_lock:
            if force_disk or len(self.memory_cache) >= self.max_memory_size:
                # 存储到磁盘
                self._save_to_disk(cache_key, value)
            else:
                # 存储到内存
                self._add_to_memory_cache(cache_key, value)
    
    def _add_to_memory_cache(self, cache_key: str, value: Any):
        """添加到内存缓存"""
        if len(self.memory_cache) >= self.max_memory_size:
            # 删除最旧的项
            self.memory_cache.popitem(last=False)
        
        self.memory_cache[cache_key] = value
    
    def _save_to_disk(self, cache_key: str, value: Any):
        """保存到磁盘"""
        if len(self.disk_index) >= self.max_disk_size:
            # 删除最旧的磁盘缓存
            oldest_key = next(iter(self.disk_index))
            oldest_file = self.cache_dir / f"{oldest_key}.pkl"
            oldest_file.unlink(missing_ok=True)
            del self.disk_index[oldest_key]
        
        file_path = self.cache_dir / f"{cache_key}.pkl"
        try:
            with open(file_path, 'wb') as f:
                pickle.dump(value, f)
            self.disk_index[cache_key] = True
            self._save_disk_index()
        except Exception as e:
            print(f"Failed to save cache to disk: {e}")
    
    def _load_disk_index(self) -> Dict[str, bool]:
        """加载磁盘缓存索引"""
        if self.disk_index_file.exists():
            try:
                with open(self.disk_index_file, 'rb') as f:
                    return pickle.load(f)
            except Exception:
                pass
        return {}
    
    def _save_disk_index(self):
        """保存磁盘缓存索引"""
        try:
            with open(self.disk_index_file, 'wb') as f:
                pickle.dump(self.disk_index, f)
        except Exception as e:
            print(f"Failed to save disk index: {e}")
    
    def clear(self):
        """清空所有缓存"""
        with self.cache_lock:
            self.memory_cache.clear()
            
            # 删除磁盘缓存文件
            for cache_key in self.disk_index:
                file_path = self.cache_dir / f"{cache_key}.pkl"
                file_path.unlink(missing_ok=True)
            
            self.disk_index.clear()
            self.disk_index_file.unlink(missing_ok=True)
    
    def get_cache_stats(self) -> Dict[str, int]:
        """获取缓存统计信息"""
        return {
            'memory_items': len(self.memory_cache),
            'disk_items': len(self.disk_index),
            'memory_limit': self.max_memory_size,
            'disk_limit': self.max_disk_size
        }
```

## 10. 部署和使用指南

### 10.1 环境安装脚本

```bash
#!/bin/bash
# setup.sh - 环境安装脚本

echo "Setting up Multi-modal Consistency Defense Environment..."

# 创建conda环境
conda create -n mm_defense python=3.9 -y
conda activate mm_defense

# 安装PyTorch（根据CUDA版本调整）
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

# 安装基础依赖
pip install transformers==4.30.0
pip install diffusers==0.18.0
pip install accelerate==0.20.0
pip install datasets==2.12.0
pip install evaluate==0.4.0

# 安装科学计算库
pip install numpy==1.24.0
pip install scipy==1.10.0
pip install scikit-learn==1.3.0
pip install pandas==2.0.0

# 安装可视化库
pip install matplotlib==3.7.0
pip install seaborn==0.12.0
pip install plotly==5.14.0

# 安装其他工具
pip install tqdm==4.65.0
pip install wandb==0.15.0
pip install tensorboard==2.13.0
pip install omegaconf==2.3.0
pip install hydra-core==1.3.0

# 安装CLIP
pip install ftfy regex
pip install git+https://github.com/openai/CLIP.git

# 创建必要目录
mkdir -p data/datasets
mkdir -p data/models
mkdir -p results/experiments
mkdir -p logs
mkdir -p cache
mkdir -p figures

echo "Environment setup completed!"
echo "Activate the environment with: conda activate mm_defense"
```

### 10.2 快速开始示例

```python
# quick_start.py - 快速开始示例

import torch
from pathlib import Path
from omegaconf import OmegaConf

# 导入我们的模块
from config.config import load_config
from defense.pipeline import create_defense_pipeline
from evaluation.evaluator import ComprehensiveEvaluator
from attacks.hubness_attack import HubnessAttacker
from tools.logger import ExperimentLogger
from tools.visualization import ExperimentVisualizer

def main():
    """快速开始示例"""
    
    # 1. 加载配置
    config = load_config('configs/default_config.yaml')
    
    # 2. 初始化日志和可视化
    logger = ExperimentLogger(
        log_dir='./logs',
        experiment_name='quick_start_demo'
    )
    visualizer = ExperimentVisualizer(save_dir='./figures')
    
    logger.log_experiment_start(OmegaConf.to_container(config))
    
    # 3. 创建防御管道
    print("Initializing defense pipeline...")
    defense_pipeline = create_defense_pipeline(config)
    
    # 4. 准备测试查询
    test_queries = [
        "A red car on the street",
        "A beautiful sunset over mountains",
        "A cat sitting on a chair",
        "Modern architecture building",
        "Ocean waves on the beach"
    ]
    
    # 5. 测试防御功能
    print("Testing defense functionality...")
    defense_results = []
    
    for query in test_queries:
        result = defense_pipeline.defend_query(query)
        defense_results.append(result)
        print(f"Query: {query}")
        print(f"  Consistency Score: {result['consistency_score']:.3f}")
        print(f"  Is Adversarial: {result['is_adversarial']}")
        print(f"  Processing Time: {result['processing_time']:.2f}s")
        print()
    
    # 6. 生成对抗样本进行测试
    print("Generating adversarial examples...")
    attacker = HubnessAttacker(config.attack)
    
    # 模拟攻击（实际使用中需要真实的图像数据库）
    adversarial_queries = []
    for query in test_queries[:3]:  # 只对前3个查询生成对抗样本
        try:
            adv_query = attacker.attack_text_query(query)
            adversarial_queries.append(adv_query)
            print(f"Original: {query}")
            print(f"Adversarial: {adv_query}")
            print()
        except Exception as e:
            print(f"Failed to generate adversarial example for '{query}': {e}")
    
    # 7. 测试对抗样本检测
    print("Testing adversarial detection...")
    adv_results = []
    
    for adv_query in adversarial_queries:
        result = defense_pipeline.defend_query(adv_query)
        adv_results.append(result)
        print(f"Adversarial Query: {adv_query}")
        print(f"  Consistency Score: {result['consistency_score']:.3f}")
        print(f"  Is Adversarial: {result['is_adversarial']}")
        print()
    
    # 8. 评估性能
    print("Evaluating performance...")
    evaluator = ComprehensiveEvaluator(config.evaluation)
    
    # 准备评估数据
    clean_scores = [r['consistency_score'] for r in defense_results]
    adv_scores = [r['consistency_score'] for r in adv_results]
    
    # 计算检测性能
    detection_metrics = evaluator.evaluate_detection_performance(
        clean_scores=clean_scores,
        adversarial_scores=adv_scores,
        threshold=config.detection.threshold
    )
    
    print("Detection Performance:")
    for metric, value in detection_metrics.items():
        print(f"  {metric}: {value:.3f}")
    
    # 9. 生成可视化
    print("Generating visualizations...")
    
    # 一致性分数分布图
    import numpy as np
    visualizer.plot_consistency_score_distribution(
        clean_scores=np.array(clean_scores),
        adv_scores=np.array(adv_scores),
        threshold=config.detection.threshold,
        save_name='quick_start_distribution.pdf'
    )
    
    # 性能对比（模拟数据）
    performance_results = {
        'Our Method': {
            'precision': detection_metrics.get('precision', 0.0),
            'recall': detection_metrics.get('recall', 0.0),
            'f1': detection_metrics.get('f1', 0.0)
        },
        'Baseline': {
            'precision': 0.7,
            'recall': 0.6,
            'f1': 0.65
        }
    }
    
    visualizer.plot_performance_comparison(
        results=performance_results,
        metrics=['precision', 'recall', 'f1'],
        save_name='quick_start_performance.pdf'
    )
    
    # 10. 记录实验结果
    final_results = {
        'defense_results': len(defense_results),
        'adversarial_results': len(adv_results),
        'detection_metrics': detection_metrics,
        'average_processing_time': np.mean([r['processing_time'] for r in defense_results])
    }
    
    logger.log_experiment_end(final_results)
    
    print("\nQuick start demo completed!")
    print(f"Results saved to: ./logs")
    print(f"Figures saved to: ./figures")
    print(f"Average processing time: {final_results['average_processing_time']:.2f}s per query")

if __name__ == "__main__":
    main()
```

     def test_batch_detection(self):
        """测试批量检测"""
        batch_vectors = np.random.randn(10, 512)
        batch_vectors = batch_vectors / np.linalg.norm(batch_vectors, axis=1, keepdims=True)
        
        self.detector.threshold = 0.5
        
        results = self.detector.batch_detect_adversarial(
            batch_vectors, self.ref_bank
        )
        
        self.assertEqual(len(results), len(batch_vectors))
        for result in results:
            self.assertIn('is_adversarial', result)
            self.assertIn('score', result)
            self.assertIn('confidence', result)

if __name__ == '__main__':
    unittest.main()
```

## 8. 性能优化建议

### 8.1 多GPU处理器

```python
# src/utils/multi_gpu_processor.py
import torch
import torch.nn as nn
from torch.nn.parallel import DataParallel, DistributedDataParallel
from typing import List, Dict, Any

class MultiGPUProcessor:
    """多GPU并行处理器"""
    
    def __init__(self, gpu_ids: List[int]):
        self.gpu_ids = gpu_ids
        self.device = torch.device(f'cuda:{gpu_ids[0]}' if gpu_ids else 'cpu')
    
    def parallelize_model(self, model: nn.Module) -> nn.Module:
        """并行化模型"""
        if len(self.gpu_ids) > 1:
            model = DataParallel(model, device_ids=self.gpu_ids)
        return model.to(self.device)
    
    def batch_process_stable_diffusion(
        self, 
        prompts: List[str], 
        batch_size: int = 4
    ) -> List[torch.Tensor]:
        """批量处理Stable Diffusion生成"""
        results = []
        
        for i in range(0, len(prompts), batch_size * len(self.gpu_ids)):
            batch_prompts = prompts[i:i + batch_size * len(self.gpu_ids)]
            
            # 分配到不同GPU
            gpu_batches = []
            for gpu_id in self.gpu_ids:
                start_idx = gpu_id * batch_size
                end_idx = min(start_idx + batch_size, len(batch_prompts))
                if start_idx < len(batch_prompts):
                    gpu_batches.append(batch_prompts[start_idx:end_idx])
            
            # 并行处理
            batch_results = self._parallel_sd_generation(gpu_batches)
            results.extend(batch_results)
        
        return results
    
    def _parallel_sd_generation(self, gpu_batches: List[List[str]]) -> List[torch.Tensor]:
        """并行SD生成（简化版）"""
        # 实际实现需要使用多进程或异步处理
        results = []
        for batch in gpu_batches:
            # 模拟SD生成
            batch_results = [torch.randn(3, 512, 512) for _ in batch]
            results.extend(batch_results)
        return results
```

### 8.2 结果缓存管理器

```python
# src/utils/cache_manager.py
import pickle
import hashlib
import os
from typing import Any, Optional
from pathlib import Path

class CacheManager:
    """结果缓存管理器"""
    
    def __init__(self, cache_dir: str = "./cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def _get_cache_key(self, *args, **kwargs) -> str:
        """生成缓存键"""
        content = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """获取缓存"""
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception:
                return None
        return None
    
    def set(self, key: str, value: Any) -> None:
        """设置缓存"""
        cache_file = self.cache_dir / f"{key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(value, f)
        except Exception:
            pass
    
    def cached_text_variants(self, query: str, num_variants: int) -> Optional[List[str]]:
        """缓存文本变体"""
        key = self._get_cache_key("text_variants", query, num_variants)
        return self.get(key)
    
    def cache_text_variants(self, query: str, num_variants: int, variants: List[str]) -> None:
        """缓存文本变体"""
        key = self._get_cache_key("text_variants", query, num_variants)
        self.set(key, variants)
    
    def cached_sd_vectors(self, prompt: str, num_images: int) -> Optional[np.ndarray]:
        """缓存SD向量"""
        key = self._get_cache_key("sd_vectors", prompt, num_images)
        return self.get(key)
    
    def cache_sd_vectors(self, prompt: str, num_images: int, vectors: np.ndarray) -> None:
        """缓存SD向量"""
        key = self._get_cache_key("sd_vectors", prompt, num_images)
        self.set(key, vectors)
```

## 9. 多GPU管理模块

### 9.1 multi_gpu_processor.py - 多GPU并行处理器

```python
from typing import List, Dict, Any, Optional, Union, Callable
import torch
import torch.nn as nn
from torch.nn.parallel import DataParallel, DistributedDataParallel
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
from dataclasses import dataclass
import numpy as np
from queue import Queue
import gc
import logging

logger = logging.getLogger(__name__)

@dataclass
class MultiGPUConfig:
    """多GPU配置"""
    gpu_ids: List[int] = None  # GPU设备ID列表
    batch_size_per_gpu: int = 4  # 每个GPU的批处理大小
    max_workers: int = 6  # 最大工作线程数
    memory_fraction: float = 0.9  # 每个GPU的内存使用比例
    enable_mixed_precision: bool = True  # 启用混合精度
    enable_compile: bool = True  # 启用torch.compile
    load_balancing: bool = True  # 启用负载均衡
    
    def __post_init__(self):
        if self.gpu_ids is None:
            self.gpu_ids = list(range(torch.cuda.device_count()))

class MultiGPUProcessor:
    """多GPU并行处理器"""
    
    def __init__(self, config: MultiGPUConfig = None):
        """
        初始化多GPU处理器
        
        Args:
            config: 多GPU配置
        """
        self.config = config or MultiGPUConfig()
        self.workers = {}
        self.executor = None
        self.load_stats = {gpu_id: 0 for gpu_id in self.config.gpu_ids}
        
        # 检查GPU可用性
        self._check_gpu_availability()
        
        # 设置GPU内存
        self._setup_gpu_memory()
        
        # 初始化线程池
        self._initialize_executor()
    
    def _setup_gpu_memory(self):
        """设置GPU内存限制"""
        try:
            for gpu_id in self.config.gpu_ids:
                torch.cuda.set_device(gpu_id)
                torch.cuda.set_per_process_memory_fraction(
                    self.config.memory_fraction, gpu_id
                )
                # 清空缓存
                torch.cuda.empty_cache()
        except Exception as e:
            logger.warning(f"设置GPU内存失败: {e}")
    
    def _check_gpu_availability(self):
        """检查GPU可用性"""
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA不可用")
        
        available_gpus = torch.cuda.device_count()
        if max(self.config.gpu_ids) >= available_gpus:
            raise ValueError(f"请求的GPU ID超出可用范围，可用GPU数量: {available_gpus}")
        
        # 检查每个GPU的状态
        for gpu_id in self.config.gpu_ids:
            try:
                torch.cuda.set_device(gpu_id)
                # 测试GPU可用性
                test_tensor = torch.randn(1, device=f'cuda:{gpu_id}')
                del test_tensor
                torch.cuda.empty_cache()
            except Exception as e:
                logger.warning(f"GPU {gpu_id} 不可用: {e}")
                self.config.gpu_ids.remove(gpu_id)
    
    def _initialize_executor(self):
        """初始化线程池执行器"""
        self.executor = ThreadPoolExecutor(
            max_workers=min(self.config.max_workers, len(self.config.gpu_ids))
        )
    
    def _get_optimal_gpu(self) -> int:
        """获取最优GPU（负载最低）"""
        if self.config.load_balancing:
            return min(self.load_stats.keys(), key=lambda x: self.load_stats[x])
        else:
            # 轮询分配
            return self.config.gpu_ids[len(self.load_stats) % len(self.config.gpu_ids)]
    
    def process_batch_parallel(self, 
                             data_batches: List[Any], 
                             process_func: Callable,
                             **kwargs) -> List[Any]:
        """
        并行处理数据批次
        
        Args:
            data_batches: 数据批次列表
            process_func: 处理函数
            **kwargs: 额外参数
        
        Returns:
            处理结果列表
        """
        if not data_batches:
            return []
        
        # 分配GPU任务
        gpu_tasks = self._distribute_tasks(data_batches)
        
        # 提交任务到线程池
        futures = []
        for gpu_id, batches in gpu_tasks.items():
            future = self.executor.submit(
                self._process_on_gpu, 
                gpu_id, batches, process_func, **kwargs
            )
            futures.append(future)
        
        # 收集结果
        results = []
        for future in as_completed(futures):
            try:
                batch_results = future.result()
                results.extend(batch_results)
            except Exception as e:
                logger.error(f"GPU处理失败: {e}")
                raise
        
        return results
    
    def _distribute_tasks(self, data_batches: List[Any]) -> Dict[int, List[Any]]:
        """分配任务到GPU"""
        gpu_tasks = {gpu_id: [] for gpu_id in self.config.gpu_ids}
        
        for i, batch in enumerate(data_batches):
            gpu_id = self.config.gpu_ids[i % len(self.config.gpu_ids)]
            gpu_tasks[gpu_id].append(batch)
        
        return gpu_tasks
    
    def _process_on_gpu(self, 
                       gpu_id: int, 
                       batches: List[Any], 
                       process_func: Callable,
                       **kwargs) -> List[Any]:
        """在指定GPU上处理批次"""
        try:
            # 设置GPU设备
            torch.cuda.set_device(gpu_id)
            
            # 更新负载统计
            self.load_stats[gpu_id] += len(batches)
            
            results = []
            for batch in batches:
                # 将数据移动到GPU
                if isinstance(batch, torch.Tensor):
                    batch = batch.to(f'cuda:{gpu_id}')
                
                # 执行处理函数
                result = process_func(batch, device=f'cuda:{gpu_id}', **kwargs)
                results.append(result)
            
            # 更新负载统计
            self.load_stats[gpu_id] -= len(batches)
            
            return results
            
        except Exception as e:
            logger.error(f"GPU {gpu_id} 处理失败: {e}")
            self.load_stats[gpu_id] -= len(batches)
            raise
        finally:
            # 清理GPU缓存
            torch.cuda.empty_cache()
    
    def encode_texts_parallel(self, 
                            texts: List[str], 
                            model, 
                            batch_size: Optional[int] = None) -> torch.Tensor:
        """
        并行编码文本
        
        Args:
            texts: 文本列表
            model: 编码模型
            batch_size: 批处理大小
        
        Returns:
            编码后的特征张量
        """
        if batch_size is None:
            batch_size = self.config.batch_size_per_gpu
        
        # 分批处理
        text_batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]
        
        def encode_batch(batch, device, **kwargs):
            with torch.no_grad():
                return model.encode_text(batch, device=device)
        
        # 并行处理
        results = self.process_batch_parallel(text_batches, encode_batch)
        
        # 合并结果
        return torch.cat(results, dim=0)
    
    def generate_images_parallel(self, 
                               prompts: List[str], 
                               model,
                               **generation_kwargs) -> List[Any]:
        """
        并行生成图像
        
        Args:
            prompts: 提示词列表
            model: 生成模型
            **generation_kwargs: 生成参数
        
        Returns:
            生成的图像列表
        """
        # 分批处理
        prompt_batches = [prompts[i:i+self.config.batch_size_per_gpu] 
                         for i in range(0, len(prompts), self.config.batch_size_per_gpu)]
        
        def generate_batch(batch, device, **kwargs):
            return model.generate_batch(batch, device=device, **kwargs)
        
        # 并行处理
        results = self.process_batch_parallel(prompt_batches, generate_batch, **generation_kwargs)
        
        # 展平结果
        flattened_results = []
        for batch_results in results:
            flattened_results.extend(batch_results)
        
        return flattened_results
    
    def get_gpu_stats(self) -> Dict[str, Any]:
        """获取GPU统计信息"""
        stats = {
            'gpu_count': len(self.config.gpu_ids),
            'gpu_ids': self.config.gpu_ids,
            'load_stats': self.load_stats.copy(),
            'memory_stats': {}
        }
        
        for gpu_id in self.config.gpu_ids:
            try:
                allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3  # GB
                reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3   # GB
                stats['memory_stats'][f'gpu_{gpu_id}'] = {
                    'allocated_gb': allocated,
                    'reserved_gb': reserved
                }
            except Exception:
                stats['memory_stats'][f'gpu_{gpu_id}'] = {'error': 'Unable to get stats'}
        
        return stats
    
    def cleanup(self):
        """清理资源"""
        if self.executor:
            self.executor.shutdown(wait=True)
        
        # 清理GPU缓存
        for gpu_id in self.config.gpu_ids:
            try:
                torch.cuda.set_device(gpu_id)
                torch.cuda.empty_cache()
            except Exception:
                pass

# 全局多GPU处理器实例
_global_processor = None
_processor_lock = threading.Lock()

def get_global_processor(config: Optional[MultiGPUConfig] = None) -> MultiGPUProcessor:
    """获取全局多GPU处理器"""
    global _global_processor
    
    if _global_processor is None:
        with _processor_lock:
            if _global_processor is None:
                _global_processor = MultiGPUProcessor(config)
    
    return _global_processor

def cleanup_global_processor():
    """清理全局多GPU处理器"""
    global _global_processor
    
    if _global_processor is not None:
        with _processor_lock:
            if _global_processor is not None:
                _global_processor.cleanup()
                _global_processor = None
```

### 9.2 multi_gpu_sd_manager.py - 多GPU Stable Diffusion管理器

```python
from typing import List, Dict, Any, Optional, Union, Tuple
import torch
import torch.nn as nn
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from dataclasses import dataclass
from PIL import Image
import time
import gc
from queue import Queue, Empty
import logging

from .sd_model import StableDiffusionModel, StableDiffusionConfig
from ..utils.multi_gpu_processor import MultiGPUProcessor, MultiGPUConfig

logger = logging.getLogger(__name__)

@dataclass
class MultiGPUSDConfig:
    """多GPU SD配置"""
    base_sd_config: StableDiffusionConfig  # 基础SD配置
    gpu_ids: List[int] = None  # GPU设备ID列表
    models_per_gpu: int = 1  # 每个GPU的模型数量
    max_concurrent_generations: int = 12  # 最大并发生成数
    load_balancing: bool = True  # 启用负载均衡
    memory_optimization: bool = True  # 启用内存优化
    
    def __post_init__(self):
        if self.gpu_ids is None:
            self.gpu_ids = list(range(torch.cuda.device_count()))

class GPUSDWorker:
    """单个GPU上的SD工作器"""
    
    def __init__(self, gpu_id: int, config: MultiGPUSDConfig, worker_id: int = 0):
        self.gpu_id = gpu_id
        self.worker_id = worker_id
        self.config = config
        self.device = torch.device(f'cuda:{gpu_id}')
        self.sd_model = None
        self.is_busy = False
        self.generation_count = 0
        
        # 初始化SD模型
        self._initialize_model()
    
    def _initialize_model(self):
        """初始化SD模型"""
        try:
            torch.cuda.set_device(self.gpu_id)
            
            # 创建SD模型配置副本
            sd_config = self.config.base_sd_config
            sd_config.device = f'cuda:{self.gpu_id}'
            
            # 初始化模型
            self.sd_model = StableDiffusionModel(sd_config)
            
            logger.info(f"GPU {self.gpu_id} Worker {self.worker_id} 初始化完成")
            
        except Exception as e:
            logger.error(f"GPU {self.gpu_id} Worker {self.worker_id} 初始化失败: {e}")
            raise
    
    def generate_single_image(self, prompt: str, **kwargs) -> Image.Image:
        """生成单张图像"""
        if self.is_busy:
            raise RuntimeError(f"Worker {self.gpu_id}-{self.worker_id} 正忙")
        
        try:
            self.is_busy = True
            torch.cuda.set_device(self.gpu_id)
            
            # 生成图像
            image = self.sd_model.generate_image(prompt, **kwargs)
            self.generation_count += 1
            
            return image
            
        finally:
            self.is_busy = False
            if self.config.memory_optimization:
                torch.cuda.empty_cache()
    
    def generate_batch_images(self, prompts: List[str], **kwargs) -> List[Image.Image]:
        """批量生成图像"""
        if self.is_busy:
            raise RuntimeError(f"Worker {self.gpu_id}-{self.worker_id} 正忙")
        
        try:
            self.is_busy = True
            torch.cuda.set_device(self.gpu_id)
            
            # 批量生成
            images = self.sd_model.batch_generate_images(prompts, **kwargs)
            self.generation_count += len(prompts)
            
            return images
            
        finally:
            self.is_busy = False
            if self.config.memory_optimization:
                torch.cuda.empty_cache()
    
    def get_stats(self) -> Dict[str, Any]:
        """获取工作器统计信息"""
        return {
            'gpu_id': self.gpu_id,
            'worker_id': self.worker_id,
            'is_busy': self.is_busy,
            'generation_count': self.generation_count,
            'memory_allocated': torch.cuda.memory_allocated(self.gpu_id) / 1024**2,  # MB
            'memory_reserved': torch.cuda.memory_reserved(self.gpu_id) / 1024**2     # MB
        }
    
    def cleanup(self):
        """清理资源"""
        try:
            if self.sd_model:
                del self.sd_model
                self.sd_model = None
            
            torch.cuda.set_device(self.gpu_id)
            torch.cuda.empty_cache()
            gc.collect()
            
        except Exception as e:
            logger.warning(f"Worker {self.gpu_id}-{self.worker_id} 清理失败: {e}")

class MultiGPUSDManager:
    """多GPU Stable Diffusion管理器"""
    
    def __init__(self, config: MultiGPUSDConfig):
        """
        初始化多GPU SD管理器
        
        Args:
            config: 多GPU SD配置
        """
        self.config = config
        self.workers = []
        self.worker_queue = Queue()
        self.executor = None
        self.load_stats = {}
        
        # 检查GPU可用性
        self._check_gpu_availability()
        
        # 初始化工作器
        self._initialize_workers()
        
        # 初始化线程池
        self._initialize_executor()
    
    def _check_gpu_availability(self):
        """检查GPU可用性"""
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA不可用")
        
        available_gpus = torch.cuda.device_count()
        for gpu_id in self.config.gpu_ids:
            if gpu_id >= available_gpus:
                raise ValueError(f"GPU {gpu_id} 不可用，可用GPU数量: {available_gpus}")
    
    def _initialize_workers(self):
        """初始化所有工作器"""
        logger.info(f"初始化 {len(self.config.gpu_ids)} 个GPU上的SD工作器...")
        
        for gpu_id in self.config.gpu_ids:
            for worker_id in range(self.config.models_per_gpu):
                try:
                    worker = GPUSDWorker(gpu_id, self.config, worker_id)
                    self.workers.append(worker)
                    self.worker_queue.put(worker)
                    
                    # 初始化负载统计
                    key = f"{gpu_id}-{worker_id}"
                    self.load_stats[key] = 0
                    
                except Exception as e:
                    logger.error(f"初始化GPU {gpu_id} Worker {worker_id} 失败: {e}")
        
        logger.info(f"成功初始化 {len(self.workers)} 个SD工作器")
    
    def _initialize_executor(self):
        """初始化线程池"""
        self.executor = ThreadPoolExecutor(
            max_workers=min(self.config.max_concurrent_generations, len(self.workers))
        )
    
    def _get_available_worker(self, timeout: float = 30.0) -> Optional[GPUSDWorker]:
        """获取可用工作器"""
        try:
            return self.worker_queue.get(timeout=timeout)
        except Empty:
            return None
    
    def _return_worker(self, worker: GPUSDWorker):
        """归还工作器"""
        self.worker_queue.put(worker)
    
    def generate_single_image(self, prompt: str, **kwargs) -> Image.Image:
        """生成单张图像"""
        worker = self._get_available_worker()
        if worker is None:
            raise RuntimeError("没有可用的SD工作器")
        
        try:
            return worker.generate_single_image(prompt, **kwargs)
        finally:
            self._return_worker(worker)
    
    def generate_images_parallel(self, prompts: List[str], **kwargs) -> List[Image.Image]:
        """并行生成多张图像"""
        if not prompts:
            return []
        
        # 分批处理
        batch_size = max(1, len(prompts) // len(self.workers))
        prompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]
        
        # 提交任务
        futures = []
        for batch in prompt_batches:
            future = self.executor.submit(self._generate_batch_with_worker, batch, **kwargs)
            futures.append(future)
        
        # 收集结果
        results = []
        for future in as_completed(futures):
            try:
                batch_results = future.result()
                results.extend(batch_results)
            except Exception as e:
                logger.error(f"批量生成失败: {e}")
                raise
        
        return results
    
    def _generate_batch_with_worker(self, prompts: List[str], **kwargs) -> List[Image.Image]:
        """使用工作器生成批量图像"""
        worker = self._get_available_worker()
        if worker is None:
            raise RuntimeError("没有可用的SD工作器")
        
        try:
            if len(prompts) == 1:
                return [worker.generate_single_image(prompts[0], **kwargs)]
            else:
                return worker.generate_batch_images(prompts, **kwargs)
        finally:
            self._return_worker(worker)
    
    def get_stats(self) -> Dict[str, Any]:
        """获取管理器统计信息"""
        worker_stats = [worker.get_stats() for worker in self.workers]
        
        total_generations = sum(stats['generation_count'] for stats in worker_stats)
        busy_workers = sum(1 for stats in worker_stats if stats['is_busy'])
        
        gpu_memory = {}
        for gpu_id in self.config.gpu_ids:
            try:
                allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3  # GB
                reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3   # GB
                gpu_memory[f'gpu_{gpu_id}'] = {
                    'allocated_gb': allocated,
                    'reserved_gb': reserved
                }
            except Exception:
                gpu_memory[f'gpu_{gpu_id}'] = {'error': 'Unable to get stats'}
        
        return {
            'total_workers': len(self.workers),
            'busy_workers': busy_workers,
            'available_workers': len(self.workers) - busy_workers,
            'total_generations': total_generations,
            'gpu_memory': gpu_memory,
            'worker_stats': worker_stats
        }
    
    def cleanup(self):
        """清理所有资源"""
        logger.info("开始清理多GPU SD管理器...")
        
        # 关闭线程池
        if self.executor:
            self.executor.shutdown(wait=True)
        
        # 清理所有工作器
        for worker in self.workers:
            try:
                worker.cleanup()
            except Exception as e:
                logger.warning(f"清理工作器失败: {e}")
        
        # 清空队列
        while not self.worker_queue.empty():
            try:
                self.worker_queue.get_nowait()
            except Empty:
                break
        
        logger.info("多GPU SD管理器清理完成")

def create_multi_gpu_sd_manager(base_config: StableDiffusionConfig,
                               gpu_ids: Optional[List[int]] = None,
                               models_per_gpu: int = 1) -> MultiGPUSDManager:
    """
    创建多GPU SD管理器的便捷函数
    
    Args:
        base_config: 基础SD配置
        gpu_ids: GPU设备ID列表
        models_per_gpu: 每个GPU的模型数量
    
    Returns:
        多GPU SD管理器实例
    """
    config = MultiGPUSDConfig(
        base_sd_config=base_config,
        gpu_ids=gpu_ids,
        models_per_gpu=models_per_gpu
    )
    
    return MultiGPUSDManager(config)
```

## 10. 部署和使用指南

### 9.1 环境安装脚本

```bash
#!/bin/bash
# setup.sh

echo "Setting up Multi-Modal Adversarial Detection Environment..."

# 创建conda环境
conda create -n mmad python=3.9 -y
conda activate mmad

# 安装PyTorch
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

# 安装基础依赖
pip install transformers==4.35.0
pip install diffusers==0.21.4
pip install accelerate==0.24.1
pip install datasets==2.14.6
pip install scikit-learn==1.3.2
pip install matplotlib==3.8.2
pip install seaborn==0.13.0
pip install tqdm==4.66.1
pip install wandb==0.16.0
pip install tensorboard==2.15.1

# 安装CLIP
pip install git+https://github.com/openai/CLIP.git

# 安装评估工具
pip install pycocotools
pip install nltk

# 创建必要目录
mkdir -p data/coco
mkdir -p data/flickr30k
mkdir -p models
mkdir -p cache
mkdir -p results
mkdir -p logs

echo "Environment setup completed!"
echo "Please activate the environment with: conda activate mmad"
```

### 9.2 快速开始示例

```python
# quick_start.py
from src.pipeline import DefensePipeline
from src.config import ExperimentConfig

def main():
    """快速开始示例"""
    
    # 加载配置
    config = ExperimentConfig.from_yaml("configs/default.yaml")
    
    # 初始化防御管道
    pipeline = DefensePipeline(config)
    
    # 示例查询
    test_queries = [
        "a cat sitting on a chair",
        "dog running in the park",
        "beautiful sunset over mountains"
    ]
    
    print("Initializing defense pipeline...")
    pipeline.initialize()
    
    print("Processing queries...")
    for query in test_queries:
        print(f"\nProcessing: {query}")
        
        # 执行检索和检测
        results = pipeline.process_query(
            query=query,
            top_k=10,
            return_scores=True
        )
        
        print(f"Retrieved {len(results['candidates'])} candidates")
        print(f"Detected {sum(r['is_adversarial'] for r in results['detection_results'])} adversarial samples")
        
        # 显示前3个结果
        for i, (candidate, detection) in enumerate(zip(
            results['candidates'][:3], 
            results['detection_results'][:3]
        )):
            status = "ADVERSARIAL" if detection['is_adversarial'] else "CLEAN"
            print(f"  {i+1}. Score: {detection['score']:.3f} | Status: {status}")

if __name__ == "__main__":
    main()
```

## 10. 总结

本文档提供了基于《Adversarial Hubness in Multi-Modal Retrieval》论文的防御方法完整实现方案，包括：

### 10.1 核心特性
- **模块化设计**：高内聚低耦合的代码架构
- **完整接口**：详细的函数签名和参数说明
- **测试覆盖**：全面的单元测试和集成测试
- **性能优化**：多GPU并行和智能缓存
- **易用性**：简单的配置和快速开始指南

### 10.2 实验流程
1. 文本变体生成和筛选
2. 多模态检索和向量收集
3. Stable Diffusion参考生成
4. 参考向量库构建
5. 对抗样本检测和过滤
6. 性能评估和报告生成

### 10.3 评估指标
- **检索性能**：Recall@K, mAP, MRR, NDCG
- **检测效果**：TPR, FPR, AUROC, 精确率/召回率
- **效率开销**：GPU内存使用、推理延迟
- **统计显著性**：Bootstrap置信区间

### 10.4 可扩展性
- 支持多种文本变体生成策略
- 可配置的检测阈值方法
- 灵活的聚合和评估策略
- 模块化的攻击方法集成

该框架为多模态检索系统的对抗防御研究提供了完整的实验平台，支持快速原型开发和大规模实验验证。